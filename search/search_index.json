{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Refget specifications"},{"location":"#what-is-refget","title":"What is refget?","text":"<p>Refget is a set of GA4GH standards for identifying and distributing reference biological sequences. It consists of these standards:</p> Standard Description Status Refget Sequences For individual sequences  v1.0 Approved in 2021 \u00a0v2.0\u00a0Approved in 2024 Refget Sequence Collections For collections of sequences  v1.0 Approved in 2025 Refget Pangenomes For collections of sequence collections  Currently in process"},{"location":"#what-is-the-main-purpose-of-the-refget-project","title":"What is the main purpose of the refget project?","text":"<p>Refget standards help to identify, retrieve, and compare reference sequences, like a reference genome. Key principles include:</p> <ul> <li>Reference data, including sequences and collections of sequences, are identified using cryptographic digest-based identifiers that are derived from the data itself. This allows reference data to be identified without requiring a centralized accessioning authority.</li> <li>Refget standards can be used for any type of sequences: DNA, RNA, protein, etc -- anything that can be represented as a string of characters.</li> <li>Refget standards also specify retrieval APIs, providing a mechanism for retrieving a sequence or collection if you have its identifier.</li> <li>Refget sequence collections also provides a programmatic approach to assessing compatibility among sequence collections.</li> </ul> <p>For more information about use cases, see the use cases section of the Sequence Collections specification.</p>"},{"location":"#how-do-the-standards-work-together","title":"How do the standards work together?","text":"<p>The Refget Sequences standard is used by the Sequence Collections standard, and the Sequence Collections standard forms the basis of the Pangenomes standard. First, sequences are digested to yield a deterministic identifier. These sequence identifiers are then used, together with their names, to create an identifier for a collection.</p>"},{"location":"citation/","title":"Citation information","text":"<p>If you use any of the refget standards or software in your work, please cite us.</p>"},{"location":"citation/#refget-sequences","title":"Refget sequences","text":"<p>Please cite:</p> <p>Yates AD; Adams J; Chaturvedi S; Davies RM; Laird M; Leinonen R; Nag R; Sheffield NC; Hofmann O &amp; Keane TM. Refget: standardized access to reference sequences. Bioinformatics, 2022, 38, 299-300. DOI: 10.1093/bioinformatics/btab524</p>"},{"location":"citation/#sequence-collections","title":"Sequence collections","text":"<p>Sequence collections publication is pending. An initial implementation that formed the groundwork for the current standard was described in:</p> <p>Stolarczyk M; Xue B; Sheffield NC. Identity and compatibility of reference genome resources. NAR Genomics and Bioinformatics, Volume 3, Issue 2, June 2021, lqab036. DOI: https://doi.org/10.1093/nargab/lqab036</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome more participants! If you are interested in contributing, one of the best ways is to raise an issue using the GitHub issue tracker. Please feel free to reach out via e-mail, or join the biweekly GA4GH calls.</p>"},{"location":"contributing/#maintainers","title":"Maintainers","text":"<ul> <li>Nathan Sheffield, Department of Genome Sciences, University of Virginia</li> <li>Andy Yates, EMBL-EBI</li> <li>Timothee Cezard, EMBL-EBI</li> </ul>"},{"location":"contributing/#contributors","title":"Contributors","text":"<ul> <li>Sveinung Gundersen, ELIXIR Norway/Centre for Bioinformatics, University of Oslo</li> <li>Add your name here!</li> </ul>"},{"location":"contributing/#call-info","title":"Call info","text":"<p>The community calls are held via zoom every other week. For information on schedule and zoom links, please contact the GA4GH Secretariat. The contact for the Large Scale Genomics work stream is Reggan Thomas (reggan.thomas@ga4gh.org).</p>"},{"location":"contributing/#ga4gh-info","title":"GA4GH info","text":"<p>The sequence collections team is a sub-group of the Large Scale Genomics Workstream. You can find more about workstreams and find the public minutes of all previous meetings at GA4GH workstreams.</p>"},{"location":"decision_record/","title":"Architectural Decision Record","text":"<p>This is a record of decisions made during specification development. Each entry describes a decision that has been approved by the team members. Collectively, this ADR describes an institutional memory for decisions and their rationales, including known limitations. The goal is to avoid repeated discussion of previous decisions, formally acknowledge limitations, preserve and articulate reasons behind the decisions, and share this information with the broader community. </p> <p>The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p>"},{"location":"decision_record/#contents","title":"Contents","text":"<ul> <li>Architectural Decision Record<ul> <li>Contents</li> <li>2024-11-20 Level 2 return values should not return transient attributes<ul> <li>Decision</li> <li>Rationale</li> </ul> </li> <li>2024-11-20 Custom modifiers should live in the schema under the ga4gh key<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-11-13 Attributes can be designed as passthru or transient.<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-10-02 Minimal schema should now require sequences, and lengths should not be inherent.<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-10-02 The /collection and /attribute endpoints will both be REQUIRED<ul> <li>Decision</li> <li>Rationale</li> </ul> </li> <li>2024-10-02 The object_type should be singular all the time<ul> <li>Decision</li> <li>Rationale</li> </ul> </li> <li>2024-10-02 We should use query parameters for the filtered list endpoint<ul> <li>Decision</li> <li>Rationale</li> </ul> </li> <li>2024-08-08 The specification should require the /attribute endpoint<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-08-08 The /list endpoint will provide global and filtered listing of collections<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-05-16 The sorted_sequences attribute will be in the spec as an optional ancillary attribute<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-02-21 We will specify core sequence collection attributes and a process for adding new ones<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-01-10 Clarifications on the purpose and form of the JSON schema in service-info<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2024-01-06 The comparison function use more descriptive attribute names<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2023-08-25 The user-facing API will neither expect nor provide prefixes<ul> <li>Rationale</li> </ul> </li> <li>2023-08-22 - Seqcol schemas MUST specify collated attributes with a local qualifier<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2023-07-26 There will be no metadata endpoint<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2023-07-12 - Required attributes are: lengths and names<ul> <li>Decision</li> <li>Rationale</li> </ul> </li> <li>2023-07-12 Implementations SHOULD provide sorted_name_length_pairs and comparison endpoint<ul> <li>Decisions</li> <li>Algorithm for computing sorted_name_length_pairs</li> <li>Rationale and alternatives considered</li> <li>Linked issues</li> </ul> </li> <li>2023-06-14 - Internal digests SHOULD NOT be prefixed<ul> <li>Background</li> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2023-06-28 - Seqcol JSON schema defines reserved attributes without additional namespacing<ul> <li>Decision</li> <li>Rationale</li> </ul> </li> <li>2023-06-28 Details of endpoints<ul> <li>Decisions</li> <li>Rationale</li> </ul> </li> <li>2023-03-22 - Seqcol schemas MUST specify inherent attributes<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> <li>Alternatives considered</li> </ul> </li> <li>2023-02-08 - Array names SHOULD be ASCII<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2023-01-25 - The digest algorithm will be the GA4GH digest<ul> <li>Decision</li> <li>Rationale</li> <li>Limitations</li> <li>Linked issues</li> </ul> </li> <li>2023-01-12 - How sequence collection are serialized prior to digestion<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> <li>Known limitations</li> <li>Alternatives considered</li> </ul> </li> <li>2022-10-05 - Terminology decisions<ul> <li>Decision</li> <li>Linked issues</li> </ul> </li> <li>2022-06-15 - Structure for the return value of the comparison API endpoint<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> <li>Alternatives considered</li> <li>Known limitations</li> </ul> </li> <li>2022-06-15 - We will define the elements of a sequence collections using a schema<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> </ul> </li> <li>2021-12-01 - Endpoint names and structure<ul> <li>Decision</li> <li>Rationale</li> <li>Limitations</li> <li>Linked issues</li> </ul> </li> <li>2021-09-21 - Order will be recognized by digesting arrays in the given order, and unordered digests will be handled as extensions through additional attributes<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> <li>Known limitations</li> </ul> </li> <li>2021-08-25 - Sequence collection digests will reflect sequence order<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> <li>Known limitations</li> </ul> </li> <li>2021-06-30 - Use array-based data structure and multi-tiered digests<ul> <li>Decision</li> <li>Rationale</li> <li>Linked issues</li> <li>Known limitations</li> </ul> </li> <li>2021-01-20 - Use the SAM specification v1 description of sequence names<ul> <li>Decision</li> <li>Linked issues</li> <li>Known limitations</li> </ul> </li> </ul> </li> </ul>"},{"location":"decision_record/#2024-11-20-level-2-return-values-should-not-return-transient-attributes","title":"2024-11-20 Level 2 return values should not return transient attributes","text":""},{"location":"decision_record/#decision","title":"Decision","text":"<p>Level 2 return values should not return transient attributes</p>"},{"location":"decision_record/#rationale","title":"Rationale","text":"<p>We debated whether the <code>/collection?level=2</code> endpoint should do with transient attributes, because the level 2 representations are not stored. One train of thought was that it could return the level 1 representation; other is that it just includes nothing. We decided that the more pure approach would be include neither</p> <p>Another option was something like <code>?level=highest</code>, which would return level 2 representations for everything that has one, but level 1 representations for transient attributes.</p> <p>We decided that even if you don't have that information, you could just get it from the <code>?level=1</code> endpoint. Or, implementations could specify their own way</p>"},{"location":"decision_record/#2024-11-20-custom-modifiers-should-live-in-the-schema-under-the-ga4gh-key","title":"2024-11-20 Custom modifiers should live in the schema under the <code>ga4gh</code> key","text":""},{"location":"decision_record/#decision_1","title":"Decision","text":"<p>Any global custom modifiers should live under a <code>ga4gh</code> key in  the schemea. Right now, this includes <code>inherent</code>, <code>transient</code>, and <code>passthru</code>. Local modifiers (currently just <code>collated</code>) will continue to live, raw, under the attribute they describe.</p>"},{"location":"decision_record/#rationale_1","title":"Rationale","text":"<p>We want to follow the standard used in the other specs (VRS), and it also seems fine to have a place to lump together our custom modifiers. We thought we could also do this for <code>collated</code>, as a local modifier, but opt not to right now because: there's only 1, it's a boolean, and it's not actually even used for anything in the spec at the moment, it is only there because it could be nice to use for a visualization of elements in a collection. The additional complexity of another layer just for this seems pointless at this point.</p>"},{"location":"decision_record/#linked-issues","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/84</li> </ul>"},{"location":"decision_record/#2024-11-13-attributes-can-be-designed-as-passthru-or-transient","title":"2024-11-13 Attributes can be designed as <code>passthru</code> or <code>transient</code>.","text":""},{"location":"decision_record/#decision_2","title":"Decision","text":"<p>We add two new attribute qualifiers: transient and passthru.</p> <ul> <li> <p>Passthru attributes are not digested in transition from level 2 to level 1. Most attributes of the canonical (level 2) seqcol representation are digested to create the level 1 representation. But sometimes, we have an attribute for which digesting makes little sense. These attributes are passed through the transformation, so they show up on the level 1 representation in the same form as the level 2 representation. Thus, we refer to them as passthru attributes. Transient attributes</p> </li> <li> <p>Transient attributes are not retrievable from the attribute endpoint. Most attributes of the sequence collection can be retrieved through the /attribute endpoint. However, some attributes may not be retrievable. For example, this could happen for an attribute that we intend to be used primarily as an identifier. In this case, we don't necessarily want to store the original content that went into the digest into the database, because it might be redundant. We really just want the final attribute. These attributes are called transient because the content of the attribute is no longer stored and is therefore no longer retrievable.</p> </li> </ul> <p>Also, a few other related decisions we finalized: - <code>collection</code> endpoint, level 2 collection representation should exclude transient attributes. - <code>attribute</code> endpoint wouldn't provide anything for either transient or passthru attributes. - Can passthru or transient attributes be inherent? They could, but it probably doesn't really make sense. Nevertheless, there's no reason to state that they cannot be.</p>"},{"location":"decision_record/#rationale_2","title":"Rationale","text":"<p>As we worked on more advanced attributes, and with the addition of the <code>/attribute</code> endpoint, we realized these changes necessitate a bit more power for the schema to specify behavior of the attributes. For the basic seqcol attributes (names, lengths, sequences) and original endpoint, the general algorithm and basic qualifiers (required, inherent, collated) suffice to describe the representation. But some more nuanced attributes require additional qualifiers to describe their intention and how the server should be behave for the <code>/attribute</code> endpoint. For example, sorted_name_length_pairs and sorted_sequences are intended to provide alternative tailored identifiers and comparisons, and not necessarily useful for independent attribute lookup. Similarly, custom extra attributes, like author or alias, may be simple appendages that don't need the complex digesting procedure we use for the basic attributes. In order to flag such attributes in a way that can govern slightly different server expectations, we need a couple of additional advanced attribute qualifiers. For this purpose, we added the passthru and transient qualifiers.</p>"},{"location":"decision_record/#linked-issues_1","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/86</li> </ul>"},{"location":"decision_record/#2024-10-02-minimal-schema-should-now-require-sequences-and-lengths-should-not-be-inherent","title":"2024-10-02 Minimal schema should now require sequences, and lengths should not be inherent.","text":""},{"location":"decision_record/#decision_3","title":"Decision","text":"<p>We will update the minimal schema with these changes: 1. Move sequences into 'required', and 2. remove lengths from 'inherent'. So the final qualifiers would be: - required: names, lengths, and sequences - inherent: names, sequences</p>"},{"location":"decision_record/#rationale_3","title":"Rationale","text":"<p>Originally, there was a good rationale for making sequences not required, to allow for coordinate systems to be represented as a seqcol. But with the new <code>/attribute</code> endpoint, there's a better way to handle it, using <code>name_length_pairs</code> and <code>sorted_name_length_pairs</code> attributes. Then, with sequences required, it does not make sense for lengths to be inherent because they are computable from sequences. So essentially, the attribute endpoint allows us to move away from handling coordinate systems as top-level entities, and instead moves toward using the attribute endpoint for coordinate systems.</p>"},{"location":"decision_record/#linked-issues_2","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/72</li> </ul>"},{"location":"decision_record/#2024-10-02-the-collection-and-attribute-endpoints-will-both-be-required","title":"2024-10-02 The <code>/collection</code> and <code>/attribute</code> endpoints will both be <code>REQUIRED</code>","text":""},{"location":"decision_record/#decision_4","title":"Decision","text":"<p>The <code>/collection</code> and <code>/attribute</code> endpoints will both be <code>REQUIRED</code></p>"},{"location":"decision_record/#rationale_4","title":"Rationale","text":"<p>We debated whether one or both of these should drop to <code>RECOMMENDED</code>, because now we can imagine a lot of use cases that would use one but not the other. But in the end, the interoperability really needs the <code>/collection</code> endpoint, and a lot of use cases will rely on the <code>/attribute</code> endpoint, so we decided to just leave them both as <code>REQUIRED</code> to reflect their dual imporantance in an interoperable eco-system. This does not stop individual implementations from doing partial implementations, like \"We only implement the <code>/attribute</code> endpoint\", if that's all they need; it simply would prevent them from claiming that they are in full compliance of the spec; they'd just have a partial implementation, which is fine. Those services would have some level of interoperability, but would not rise to the level needed to do some of the meta-aggregation we can imagine, so we feel it's appropriate for them to only claim partial compliance.</p>"},{"location":"decision_record/#2024-10-02-the-object_type-should-be-singular-all-the-time","title":"2024-10-02 The <code>object_type</code> should be singular all the time","text":""},{"location":"decision_record/#decision_5","title":"Decision","text":"<p>The endpoints that can be moduled by <code>object_type</code>, <code>/list/:object_type</code> and <code>/attribute/:object_type</code>, should always use the singular form of the object_type.</p>"},{"location":"decision_record/#rationale_5","title":"Rationale","text":"<p>It's easier if we have this be uniform, instead of having <code>/list/collections</code> and then <code>/attribute/collection</code> and then defining these both as <code>object_type</code>; to be strictly accurate here we'd need to define a second variables, like <code>plural_object_type</code>, so that the spec would be internally consistent. Instead, we don't really see a disadvantage to just making <code>object_type</code> have a consistent definition, so that it can be reused throughout the spec. So the end point should change to <code>/list/collection</code>.</p>"},{"location":"decision_record/#2024-10-02-we-should-use-query-parameters-for-the-filtered-list-endpoint","title":"2024-10-02 We should use query parameters for the filtered list endpoint","text":""},{"location":"decision_record/#decision_6","title":"Decision","text":"<p>The filtered list endpoint should filter by adding query parameters to the unfiltered endpoint, like <code>/list/:object_type?:attribute1=:attribute_digest1&amp;attribute2=:attribute_digest2</code>.</p>"},{"location":"decision_record/#rationale_6","title":"Rationale","text":"<p>Originally, we had defined two path-based variants of the list endpoint; unfiltered as <code>/list/collection</code> and filtered as <code>/list/collection/:attribute/:attribute_digest</code>. We realized this has some disadvantages; first, it requires us to define these as two separate endpoints, and second, it makes it so you can't enable filtering by more than one attribute digest. We didn't really see a disadvantage to just switching to optional query parameters, and we see several advantages. Now everything fits nicely under a single endpoint definition, and it's natural that without a filter parameter, you simply give the unfiltered result, but with the filter parameter, you give the filtered result. Furthermore, it sets the stage for multiple values, if this could be useful.</p>"},{"location":"decision_record/#2024-08-08-the-specification-should-require-the-attribute-endpoint","title":"2024-08-08 The specification should require the <code>/attribute</code> endpoint","text":""},{"location":"decision_record/#decision_7","title":"Decision","text":"<p>We decided to add to the specification the <code>/attribute</code> endpoint, which would retrieve values of given collection attributes given attribute digests. This is parallel to the <code>/collection</code> endpoint, which retrieves the whole collection given a top-level digest.</p>"},{"location":"decision_record/#rationale_7","title":"Rationale","text":"<p>Several use cases have re-emphasized the value of the digests of the <code>sorted_sequences</code>, the <code>sorted_name_length_pairs</code>, and other collection attributes. For many use cases, these are really the most important piece of information. However, until now, we've pushed for dealing with top-level collections, and using these as information contained within a collection. This has driven people to want to create separate schemas, which will hurt long-term interoperability. Instead, we realized that if we elevated the status of the attributes, such that users could retrieve these values directly, then that would allow these use cases to live within the ecosystem without needing to specify separate schemas. This change will therefore allow us to preserve some interoperability.</p>"},{"location":"decision_record/#linked-issues_3","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/80</li> <li>https://github.com/ga4gh/refget/issues/77</li> </ul>"},{"location":"decision_record/#2024-08-08-the-list-endpoint-will-provide-global-and-filtered-listing-of-collections","title":"2024-08-08 The <code>/list</code> endpoint will provide global and filtered listing of collections","text":""},{"location":"decision_record/#decision_8","title":"Decision","text":"<p>We decided to include the <code>/list</code> endpoint in two variants, a global one that just lists all available collections, and a filtered one, that allows users to list any collections that have a certain attribute. It should be <code>/list/collections</code>, in anticipation of future endpoints that could list entities of other types (like pangenomes or attributes)</p>"},{"location":"decision_record/#rationale_8","title":"Rationale","text":"<p>We had been brainstorming about listing and filtered listing endpoints for several years, and it was always on the roadmap. We could think of clear use cases. For example, it would be necessary for a meta-service that would aggregate across sequence collections as a way to discover what is contained in one. We had also for along time debated a discovery endpoint that would allow searching through sequence collections. We were originally going to postpone this to v1.1, but in recent months it's become clear that these features are really important to drive uptake of the standard.</p>"},{"location":"decision_record/#linked-issues_4","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/61</li> <li>https://github.com/ga4gh/refget/issues/28</li> <li>https://github.com/ga4gh/refget/issues/27</li> </ul>"},{"location":"decision_record/#2024-05-16-the-sorted_sequences-attribute-will-be-in-the-spec-as-an-optional-ancillary-attribute","title":"2024-05-16 The <code>sorted_sequences</code> attribute will be in the spec as an optional ancillary attribute","text":""},{"location":"decision_record/#decision_9","title":"Decision","text":"<p>We decided to add <code>sorted_sequences</code> to the spec as OPTIONAL.</p>"},{"location":"decision_record/#rationale_9","title":"Rationale","text":"<p>When digested, this attribute provides a digest representing an order-invariant set of unnamed sequences. It provides a way to compare two sequence collections to see if their sequence content is identical, but just in a different order. Such a comparison can be made by the comparison function, so why might you want to include this attribute as well? In some large-scale use cases, comparing the sequence content without considering order is something that needs to be done repeatedly and for a huge number of collections. In these cases, using the comparison function could be computationally prohibitive. This digest allows the comparison to be pre-computed, and more easily compared.</p> <p>This attribute has been suggested by users for different use cases, and it provides a good example of an ancillary attribute that could be useful for a specific use case where you want to pre-compute this comparison instead of relying on the comparison function. Thus, it makes sense to include as an example, but made optional since many use cases will not need it.</p> <p>In the future if the number of proposed ancillary attributes grows, it could move to a separate document together with other ideas for ancillary attributes.</p>"},{"location":"decision_record/#linked-issues_5","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/71</li> </ul>"},{"location":"decision_record/#2024-02-21-we-will-specify-core-sequence-collection-attributes-and-a-process-for-adding-new-ones","title":"2024-02-21 We will specify core sequence collection attributes and a process for adding new ones","text":""},{"location":"decision_record/#decision_10","title":"Decision","text":"<p>The sequence collection specification will sanction a number of attributes for which a clear and commonly accepted definition will be provided. These attributes will be defined in the sequence collection schema and will be part of the specification.</p> <p>Additional attributes can be requested to be added to the schema via opening an issue on the sequence collection specification GitHub repo. These will be labeled \"schema-term\".</p> <p>The set of open issues with this tag can be viewed as an extended seqcol schema that includes all attributes proposed by the community. We RECOMMEND implementers monitor this set to increase forward interoperability.</p>"},{"location":"decision_record/#rationale_10","title":"Rationale","text":"<p>It is important for the interoperability of services that attributes used in different implementations have the same definition. To ensure this, the centrally defined schema will provide clear definitions of the most important attributes.  However it is clear that the maintainers cannot define all possible attributes that implementations might need, so it became apparent that an extended list of attributes that have not been fully defined yet would be useful.</p> <p>Choosing to host this list as a list of issues allows the list to always be up to date and also contain comment threads where the community can discuss the definition and approval of each attribute.</p>"},{"location":"decision_record/#linked-issues_6","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/50</li> <li>https://github.com/ga4gh/refget/issues/46</li> <li>https://github.com/ga4gh/refget/issues?q=is%3Aissue+is%3Aopen+label%3Aschema-term</li> </ul>"},{"location":"decision_record/#2024-01-10-clarifications-on-the-purpose-and-form-of-the-json-schema-in-service-info","title":"2024-01-10 Clarifications on the purpose and form of the JSON schema in service-info","text":""},{"location":"decision_record/#decision_11","title":"Decision","text":"<p>We made a series of decisions regarding how the JSON-schema should be used to specify the data that a seqcol server will serve.</p> <ul> <li>you MUST provide a single schema.</li> <li>the provided schema MUST include all possible attributes your service may provide (you cannot have a collection with an attribute that is not defined in your schema).</li> <li>the seqcol spec will include a central, approved seqcol schema</li> <li>for any terms defined by the central, approved seqcol schema, any implementations MUST use definitions provided, either by using refs or by duplicating the terms</li> <li>we RECOMMEND your schema use property-level refs to point to terms defined by a central, approved seqcol schema, rather than duplicating terms</li> <li>we RECOMMEND your schema only define terms actually used in at least one collection you serve.</li> </ul>"},{"location":"decision_record/#rationale_11","title":"Rationale","text":"<p>This set of decisions is oriented around solving a series of related problems.</p> <p>A JSON schema really serves multiple purposes: 1. validation; 2. configuration of a server; 3. providing information to users about what a server does. Here, the JSON-schema in the service info is really primarily for the third point.</p> <p>Allowing the JSON schema to use refs introduces some challenges, because now a third-party using that schema will need to resolve those refs. While an existing JSON-schema validator would have a built-in dereferencer, currently, these are only useful if you're using the JSON-schema for validation, which may not be the goal. So, we acknowledge that allowing refs in the JSON schema has potential to make it a bit harder use; but the benefit of being able to share definitions is too great to ignore. The goal here is to increase sharability, and this will be done most effectively if users can easily point to other JSON schemas -- in particular, a central, approved one that is released with recommended terms as part of the seqcol spec.</p> <p>Another issue is that we wanted the schema to be a place where a user could see what the shape of the data in the server would look like, but we realized this is basically impossible, because different collections in a given server could possibly have different attributes, and therefore, different schemas. Therefore, the only thing that makes sense is for the schema served by the service-info endpoint to have all possible attributes that could be included in any collection hosted by a particular server. This way, you're at least guaranteed that you won't encounter an attribute that is not defined by the schema, though we cannot guarantee that all sequence collections will contain all attributes defined in the schema.</p>"},{"location":"decision_record/#linked-issues_7","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/50</li> <li>https://github.com/ga4gh/refget/issues/39</li> </ul>"},{"location":"decision_record/#2024-01-06-the-comparison-function-use-more-descriptive-attribute-names","title":"2024-01-06 The comparison function use more descriptive attribute names","text":""},{"location":"decision_record/#decision_12","title":"Decision","text":"<p>This decision complements and updates the one taken on 2022-06-15. It changes the attribute names to describe specifically what is being returned. In the top level object:  - rename <code>arrays</code> to <code>attributes</code>, and it should describe all attributes regardless of their types.  - rename <code>elements</code> to <code>array_elements</code>, and it should describe only attribute of type arrays regardless of the <code>collated</code> attribute</p> <p>In the <code>array_elements</code> (previously <code>elements</code>): - remove the <code>total</code> and replace it with <code>a_count</code> and <code>b_count</code> where <code>a_count</code> list all the arrays from <code>a</code> and the number of element they contain and <code>b_count</code> does the same for <code>b</code>. - replace <code>a_and_b</code> with <code>a_and_b_count</code> -- the content stay the same</p>"},{"location":"decision_record/#rationale_12","title":"Rationale","text":"<p>The comparison function is designed to compare two sequence collections by interrogating the content of the collated arrays. The initial attribute names were not specifically stating that they only applied to arrays, since originally, we had only been envisioning array attributes. Now that it's more clear how non-array attributes could be included, these updates to the comparison return value clarify which attributes are being referenced.</p>"},{"location":"decision_record/#linked-issues_8","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/57</li> </ul>"},{"location":"decision_record/#2023-08-25-the-user-facing-api-will-neither-expect-nor-provide-prefixes","title":"2023-08-25 The user-facing API will neither expect nor provide prefixes","text":""},{"location":"decision_record/#rationale_13","title":"Rationale","text":"<p>We have debated whether the digests returned by the API should be prefixed in any way (either a namespace-type prefix, or a type prefix). We have also debated whether the API should accept prefixed versions of digests. We decided (for now) that neither should be true; our protocol is simply to use and provide the seqcol digests straight up. We view the prefixes as being something useful for an external context to determine where a digest came from or belongs; but this seems external to how our service should behave internally. Therefore, any sort of prefix should be happening by whatever is using this service, not by the service itself. For example, a ga4gh-wide broker that could disambiguate between different types of digests may require an incoming digest to have a type prefix, but this would be governed by such a context-oriented service, not by the seqcol service itself.</p> <p>In the future, when it becomes more clear how this service will fit in with other services in the ga4gh ecosystem, we could revisit this decision.</p>"},{"location":"decision_record/#2023-08-22-seqcol-schemas-must-specify-collated-attributes-with-a-local-qualifier","title":"2023-08-22 - Seqcol schemas MUST specify collated attributes with a local qualifier","text":""},{"location":"decision_record/#decision_13","title":"Decision","text":"<p>Collated attributes are seqcol attributes where the values of the attribute are 1-to-1 with sequences in the collection, and represented in the same order as the sequences in the collection. Names, lengths, and sequences are examples of collated attributes. While not strictly required for the core functionality of sequence collections, we anticipate downstream applications will benefit if the seqcol JSONschema specifies which attributes are collated. We therefore REQUIRE the seqcol JSONschema to specify which attributes are collated, and these will be specified using a local boolean qualifier (defined below). </p>"},{"location":"decision_record/#rationale_14","title":"Rationale","text":"<p>For applications that will visualize or present to the user a representation of a sequence collection, it will be useful to know if any attributes have one-value-per-sequence, or something else. The collated attributes are those that belong to each sequence. It is possible that other attributes will exist on the seqcol object, but a generic visualization engine or other processor will benefit from knowing which ones are collated.</p> <p>But how to specify them in the JSONschema? In jsonschema, there are 2 ways to qualify properties: 1) a local qualifier, using a key under a property; or 2) an object-level qualifier, which is specified with a keyed list of properties up one level. For example, you annotate a property's <code>type</code> with a local qualifier, underneath the property, like this:</p> <pre><code>properties:\n  names:\n    type: array\n</code></pre> <p>However, you specify that a property is <code>required</code> by adding it to an object-level <code>required</code> list that's parallel to the <code>properties</code> keyword:</p> <pre><code>properties:\n  names:\n    type: array\nrequired:\n  - names\n</code></pre> <p>In sequence collections, we chose to use define <code>collated</code> as a local qualifier. Local qualifiers fit better for qualifiers independent of the object as a whole. They are qualities of a property that persist if the property were moved onto a different object. For example, the <code>type</code> of an attribute is consistent, regardless of what object that attribute were defined on. In contrast, object-level qualfier lists fit better for qualifiers that depend on the object as a whole. They are qualities of a property that depend on the object context in which the property is defined. For example, the <code>required</code> modifier is not really meaningful except in the context of the object as a whole. A particular property could be required for one object type, but not for another, and it's really the object that induces the requirement, not the property itself.</p> <p>We reasoned that <code>inherent</code>, like <code>required</code>, describes the role of an attribute in the context of the whole object; An attribute that is inherent to one type of object need not be inherent to another. Therefore, it makes sense to treat this concept the same way jsonschema treats <code>required</code>.  In contrast, the idea of <code>collated</code> describes a property independently: Whether an attribute is collated is part of the definition of the attribute; if the attribute were moved to a different object, it would still be collated.</p> <p>For example, here the <code>lengths</code> attribute is maraked as collated using a local qualifier. The <code>author</code> attribute is marked as not collated in the same way:</p> <pre><code>description: \"A collection of biological sequences.\"\ntype: object\nproperties:\n  lengths:\n    type: array\n    collated: true\n    description: \"Number of elements, such as nucleotides or amino acids, in each sequence.\"\n    items:\n      type: integer\n  author:\n    type: string\n    collated: false\n    description: \"The author of this sequence collection\"\n...\n</code></pre>"},{"location":"decision_record/#linked-issues_9","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/40</li> </ul>"},{"location":"decision_record/#2023-07-26-there-will-be-no-metadata-endpoint","title":"2023-07-26 There will be no metadata endpoint","text":""},{"location":"decision_record/#decision_14","title":"Decision","text":"<p>We have no need for a <code>/metadata</code> endpoint</p>"},{"location":"decision_record/#rationale_15","title":"Rationale","text":"<p>At one point (issue #3), we debated whether there should be a <code>/metadata</code> endpoint or something like that as a way to retrieve information about a sequence that might not be part of the digested sequence. However, after we distinguished between <code>inherent</code> and <code>non-inherent</code> attributes, we have realized that this satisfies the earlier requirement for a <code>/metadata</code> endpoint; in fact, the metadata can be returned to the user through the normal endpoint, and just flagged as <code>non-inherent</code> in the schema to indicate that it's not digested, and therefore not part of the identity of the object</p> <p>We distinguished between two types of metadata:</p> <ul> <li>server-scoped metadata, like the schema we described above, should be served by <code>/service-info</code></li> <li>collection-scoped or sequence-scoped metadata don't fit under <code>/service-info</code>. For these, they will be served by the primary <code>/collection</code> endpoint, rather than by a separate <code>/metadata</code> endpoint.</li> </ul>"},{"location":"decision_record/#linked-issues_10","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/3</li> <li>https://github.com/ga4gh/refget/issues/39</li> <li>https://github.com/ga4gh/refget/issues/40</li> </ul>"},{"location":"decision_record/#2023-07-12-required-attributes-are-lengths-and-names","title":"2023-07-12 - Required attributes are: lengths and names","text":""},{"location":"decision_record/#decision_15","title":"Decision","text":"<p>A sequence collection consists of a set of arrays. The only arrays that MUST be included for a valid sequence collection are lengths and names. All other possible arrays, including sequences and other controlled vocabulary arrays, are not required.</p>"},{"location":"decision_record/#rationale_16","title":"Rationale","text":"<p>Debate around what should be mandatory as centered on 3 specific attributes: sequences, names, and lengths:</p> <p>At first, it feels like sequences are fundamental components of sequence collections, and therefore, the sequences array should be mandatory, and names and lengths may be superfluous. For reference genomes, for example, it's clear that collections of sequences are the main function of sequence collections. However, analysis of reference genome data also includes many analyses for which the sequences themselves do not matter, and the critical component is simply the name and length of the sequence. An array of names and lengths can be thought of as a coordinate system, and we have realized that the sequence collection specification is also extremely useful for representing and uniquely identifying coordinate systems. From this perspective, we envision a coordinate system as a sequence collection in which the actual sequence content is irrelevant, but in which the lengths and names of the sequences are critical. Analysis of coordinate systems like this is very frequent. For example, any sort of annotation analysis looking at genomic regions will rely on the lengths of the sequences to enforce that coordinates refer to the same thing, but do not rely on the underlying sequences. This is why \"chrom-sizes\" files are used so frequently (e.g. across many UCSC tools).</p> <p>This leads us to the conclusion that sequences should be optional, and names and lengths should be the only mandatory components. Lengths makes sense because if you have a sequence, you can always compute it's length, but if you don't have a sequence (all you have is a coordinate system), you may only have a length. We debated extensively whether names should be mandatory, and in the end, decided that it's unlikely to pose much of a difficulty to make it mandatory, and provides a lot of convenience. If sequences lack names altogether, it is trivial to name them by index of the order of the sequences. We reason that downstream use cases are very likely to require at least some type of identifier to refer to each of the sequences, even if it's just the index of the sequence in the list. While it may be possible to imagine a use case where an identifier for each sequence is not required, it's not difficult at all to just assign indexes. By making it required, we ensure that implementations will always have the same possible way to reference the sequences in the collection. Also, one potential use case for dropping the names array, namely to provide name-invariant sequence records for mapping purposes, will instead be possible to solve through defining an extra non-inherent and name-invariant attribute.</p>"},{"location":"decision_record/#2023-07-12-implementations-should-provide-sorted_name_length_pairs-and-comparison-endpoint","title":"2023-07-12 Implementations SHOULD provide sorted_name_length_pairs and comparison endpoint","text":""},{"location":"decision_record/#decisions","title":"Decisions","text":"<ol> <li>Name of the \"names-lengths\" attribute should be <code>sorted_name_length_pairs</code>.</li> <li>The <code>sorted_name_length_pairs</code> is RECOMMENDED.</li> <li>The <code>/comparison</code> endpoint is RECOMMENDED.</li> <li>The algorithm for computing the <code>sorted_name_length_pairs</code> attribute should be as follows:</li> </ol>"},{"location":"decision_record/#algorithm-for-computing-sorted_name_length_pairs","title":"Algorithm for computing <code>sorted_name_length_pairs</code>","text":"<ol> <li>Lump together each name-length pair from the primary collated <code>names</code> and <code>lengths</code> into an object, like <code>{\"length\":123,\"name\":\"chr1\"}</code>.</li> <li>Canonicalize JSON according to the seqcol spec (using RFC-8785).</li> <li>Digest each name-length pair string individually.</li> <li>Sort the digests lexographically.</li> <li>Add as an undigested, uncollated array to the sequence collection.</li> </ol>"},{"location":"decision_record/#rationale-and-alternatives-considered","title":"Rationale and alternatives considered","text":"<ol> <li> <p>We considered <code>names_lengths</code>, <code>sorted_names_lengths</code>, <code>name_length_pairs</code>. In the end we are trying to strike a balance between descriptiveness and conciseness. We decided the idea of \"pairs\" is really critical, and so is \"sorted\", so this seemed to us to be a minimal set of words to capture the intention of the attribute, though it is a bit long. But in the end the name itself just has to be something standardized, and nothing seems perfect.</p> </li> <li> <p>We debated whether it should be required or optional to provide the <code>sorted_name_length_pairs</code> attribute. We think it provides a lot of really nice benefits, particularly if everyone implements it; however, we also acknowledge that there are some use cases for seqcols (like just being a provider of sequence collections) where every collection will have sequences, and comparing among coordinate systems is not really in scope. For this use case, we acknowledge that the sorted_name_length_pairs may not have utility, so we make it RECOMMENDED.</p> </li> <li> <p>Similarly, we envisioned the possibility of a minimal implementation built using object storage that could fulfill all the other specifications. So while we think that the comparison function will be very helpful, particularly if it's implemented everywhere, for a minimal implementation that's sole purpose is to provide sequences, it might make sense to opt out of this. Therefore, we call it recommended.</p> </li> </ol>"},{"location":"decision_record/#linked-issues_11","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/40</li> </ul>"},{"location":"decision_record/#2023-06-14-internal-digests-should-not-be-prefixed","title":"2023-06-14 - Internal digests SHOULD NOT be prefixed","text":""},{"location":"decision_record/#background","title":"Background","text":"<p>In some situations, digests are prefixed. For example, these may be CURIEs, which specify namespaces or provide other information about what the digest represents. This raises questions about when and where we should expect or use prefixes. This has to be determined because including prefixes in the content that gets digested changes it, so we have to be consistent.</p>"},{"location":"decision_record/#decision_16","title":"Decision","text":"<p>We determined that internally, we will not append prefixes to the strings we are going to digest. However, if a particular identifier defines some kind of a prefix as part of the identifier (e.g. a refget sequence identifier), then it's of course no problem, we take that identifier at face value. To summarize:</p> <ul> <li>for internal identifiers (those generated within seqcol), we digest only digests, not prefixes of any kind</li> <li>for external identifiers (like refget identifiers), we accept them at face value, so we wouldn't remove a prefix if you declare it is was part of your sequence identifier</li> <li>the seqcol specification should RECOMMEND using refget identifiers</li> </ul> <p>More specifically, for refget, there are two types of prefix: the namespace prefix (<code>ga4gh:</code>) and type type prefix (<code>SQ.</code>). Right now, the refget server requires you to have the type prefix to request a lookup; the refget protocol declares that this type prefix is part of the identifier. However, the <code>ga4gh:</code> prefix is more of a namespace prefix and is not required, and therefore not considered part of the identifier. Therefore, the seqcol <code>sequence</code> values would include the <code>SQ.</code> but not the <code>ga4gh:</code>.</p>"},{"location":"decision_record/#rationale_17","title":"Rationale","text":"<p>According to the definition of CURIEs:</p> <p>A host language MAY declare a default prefix value, or MAY provide a mechanism for defining a defining a default prefix value. In such a host language, when the prefix is omitted from a CURIE, the default prefix value MUST be used.</p> <p>We see no need to add prefixes to the identifiers we use internally, which we just assume belong to our namespace. Adding prefixes will complicate things and does not add benefits. Prefixes may be added to our identifiers by outside entities as needed to define for them the scope of our local digests.</p>"},{"location":"decision_record/#linked-issues_12","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/37</li> </ul>"},{"location":"decision_record/#2023-06-28-seqcol-json-schema-defines-reserved-attributes-without-additional-namespacing","title":"2023-06-28 - Seqcol JSON schema defines reserved attributes without additional namespacing","text":""},{"location":"decision_record/#decision_17","title":"Decision","text":"<p>One potential issue may arise if a custom implementation uses an attribute that a future version of seqcol adds to the schema, and if these attributes are defined differently. This will create a name clash and the custom implementation wouldn't be compatible with the future seqcol schema. It would be nice to prevent such future clashes, which would require ensuring that future seqcol attributes mean the same thing across collections so they are compatible. One way to solve this would be to define an namespace reserved by the specification, so that custom attributes look different and will therefore guarantee that custom attribute never clashes with seqcol attributes, thereby ensuring that custom implementations will be compatible with any future schema updates.</p> <p>Despite the potential issue for custom attribute clashes, we decided:</p> <ol> <li> <p>We will not use any additional namespacing. Instead, the seqcol schema declares and defines the specific attributes of a sequence collection. We will \"claim\" any reserved keywords in the secqol schema we publish, not by defining a style or namespace of reserved keywords.</p> </li> <li> <p>We will try to add to this many things that we forsee as possible attributes that could be defined in a seqcol. Thus, we will provide an official set of definitions that should prevent many possible future clashes.</p> </li> <li> <p>We will specify that for custom attributes, you can do what you want outside the reserved keywords; but you should be aware that if a word becomes part of the official schema in the future, this could require a change of your custom attribute to maintain backwards compatibility. We will advise that if possibility of future clashes is important for an external schema, they could prevent that by prefixing custom attributes. However, this also means that if a future attribute is added to the schema to represent that concept, it would not follow the custom name.</p> </li> <li> <p>Third-party implementers may propose attributes that should be moved into the primary seqcol schema for subsequent release. These proposals could happen via raising an issue in the specification repository.</p> </li> </ol>"},{"location":"decision_record/#rationale_18","title":"Rationale","text":"<p>Several reasons led us to this decisions:</p> <ol> <li>The likelihood of wanting to add custom attributes that will clash and have different definition seems low, so we questioned whether it is worth the cost of defining separate namespaces.</li> <li>In the event that there is a clash in the future, this is not really a major problem. A new version of the official schema that adds new reserved keywords will basically mean a new major release of seqcol, which could potentially introduce backwards incompatibility with an existing custom attribute. This just means the custom implementation would need to be updated to follow the new schema, which is possible.</li> <li>It seems more likely that we would \"claim\" an official attribute that someone else had already used that does match the intended semantics of the word. In that case, our effort to prevent clashes would have actually created clashes, because it would have forced the custom attribute to use a different attribute name. Instead, it seems more prudent to just allow the custom implementations to use the same namespace of attribute names, and deal with any possible backwards incompatibilities if they ever actually arise in the future.</li> <li>Since we expect the major implementations to be few and driven by people connected with the project, it seems more likely that we would just adopt the custom attribute with its definition as an official attribute. We would not be able to do this if we enforced separate namespaces, which would create backwards compatibility.</li> </ol> <p>In other words, in short: the idea to prevent future backwards-incompatibility by creating a reserved word namespace seems, paradoxically, more likely to actually create a future backwards compatibility than to prevent one.</p>"},{"location":"decision_record/#2023-06-28-details-of-endpoints","title":"2023-06-28 Details of endpoints","text":""},{"location":"decision_record/#decisions_1","title":"Decisions","text":"<ol> <li> <p>The specification for how to retrieve different representations of a sequence collection should be specified to the <code>/collection</code> endpoint with <code>?level=&lt;level&gt;</code>, where <code>&lt;level&gt;</code> interpretations are:</p> <ul> <li><code>level</code> &lt;= 0 is undefined</li> <li>the return value is JSON for all </li> <li><code>?level=1</code> MUST be allowed, and must return the level 1 seqcol representation</li> <li><code>?level=2</code> MUST be allowed, and must return the level 2 seqcol representation</li> <li><code>?level</code> is OPTIONAL, and when not provided, <code>level=2</code> is assumed</li> </ul> </li> <li> <p>The <code>/comparison</code> endpoint is RECOMMENDED.</p> </li> </ol>"},{"location":"decision_record/#rationale_19","title":"Rationale","text":"<ol> <li> <p>The different levels of representation are useful for different things and it makes sense to make it possible to retrieve them. We debated about the best way to do this, and also considered using names instead of numbers.</p> </li> <li> <p>The comparison endpoint is very useful, but we can imagine use cases where it can cause problems or may not be needed. First, it will preclude the ability of creating an S3-only implementation. Since it's possible and useful to create an implementation that only implements the <code>/collection</code> endpoint, it makes sense that <code>/comparison</code> should not be required. Second, some services may view themselves as solely providing content, and nothing more. We recommend these services still implement <code>/comparison</code>, but acknowledge that the <code>/collection</code> endpoint will still be useful even without it, so this again fits with a <code>RECOMMEND</code> status.</p> </li> </ol>"},{"location":"decision_record/#2023-03-22-seqcol-schemas-must-specify-inherent-attributes","title":"2023-03-22 - Seqcol schemas MUST specify inherent attributes","text":""},{"location":"decision_record/#decision_18","title":"Decision","text":"<p>A seqcol schema provided by a seqcol API <code>MUST</code> define an <code>inherent</code> section. This section specifies a list of attributes, indicating the attributes that do contribute to the identity of the collection. As a corollary, attributes of a seqcol that are not listed in <code>inherent</code> <code>MUST NOT</code> contribute to the identifier; they are therefore excluded from the digest calculation.</p>"},{"location":"decision_record/#rationale_20","title":"Rationale","text":"<p>We have found a lot of useful use cases for information that should go along with a seqcol, but should not contribute to the identity of that seqcol. This is a useful construct as it allows us to include information in a collection that does not affect the identifier that is computed for that collection. One simple example is the \"author\" or \"uploader\" of a reference sequence; this is useful information to store alongside this collection, but we wouldn't want the same collection with two different authors to have a different identifier! Similarly, the 'sorted_name_length_pairs' idea provides lots of utility, but it doesn't change anything about the identity of the collection, so it would be nice to exclude it because then an implementation that didn't implement 'sorted_name_length_pairs' would end up with the same identifier, improving interoperability across servers.</p> <p>Thus, we introduce the idea of inherent vs non-inherent attributes. Inherent attributes contribute to the identifier; non-inherent attributes are not considered in computing the top-level digest. We previously called these digested and non-digested attributes, but this is not really a good name because, while these non-inherent attributes may not be part of the top-level digest calculation, they are still going to be digested at level 2.</p>"},{"location":"decision_record/#linked-issues_13","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/40</li> </ul>"},{"location":"decision_record/#alternatives-considered","title":"Alternatives considered","text":"<p>We considered using <code>extrinsic</code> to define the opposite of <code>inherent</code>, which would change it so that attributes were inherent by default; but we decided we liked the explicitness of forcing the schema to specify which attributes are to be included in the digest, because this brings clarity over the alternative, which is to assume everything is included unless it's excluded. We also liked that this makes the <code>inherent</code> keyword behave similarly to the <code>required</code> keyword in JSON-schema; if left off, we assume nothing is required. This means that in order for a seqcol schema to be valid, it must have at least one inherent attribute specified.</p>"},{"location":"decision_record/#2023-02-08-array-names-should-be-ascii","title":"2023-02-08 - Array names SHOULD be ASCII","text":""},{"location":"decision_record/#decision_19","title":"Decision","text":"<p>Custom array names SHOULD be ASCII characters. We expect most implementations will require this; nevertheless, implementers may choose to allow UTF-8 characters as an extension to the spec. Implementing UTF-8 is not required for an implementation. In this extension, array names MUST at least follow UTF-8.</p>"},{"location":"decision_record/#rationale_21","title":"Rationale","text":"<p>The sequence collection is a group of named arrays. These array names include built-in, defined arrays, like names, lengths, and sequences, but users may also use custom array names. Our spec-defined array names are all lowercase ASCII characters, but this doesn't mean we must restrict custom array names in the same way.</p> <p>While non-ASCII array names would be compatible with our current specification, we identified 3 issues that could arise if someone uses non-ASCII: 1) Normalization. We would probably need to define in the specification some normalization scheme to make sure things a user expects to be identical will hash to the same digest. 2) Sort order. However, this problem will be solved by following a JSON canonicalization standard. 3) Use of array names in other places will be restricted. For example, it seems natural to want to create API endpoints or table names or in columns in a database that correspond to array names. If array names are non-ASCII, it may preclude this, increasing implementation complexity and may make some things impossible.</p>"},{"location":"decision_record/#linked-issues_14","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/33</li> </ul>"},{"location":"decision_record/#2023-01-25-the-digest-algorithm-will-be-the-ga4gh-digest","title":"2023-01-25 - The digest algorithm will be the GA4GH digest","text":""},{"location":"decision_record/#decision_20","title":"Decision","text":"<p><code>sha512t24u</code> digests must be used instead of <code>md5</code> for sequence collection digests.</p>"},{"location":"decision_record/#rationale_22","title":"Rationale","text":"<p><code>sha512t24u</code> was created as part of the Variation Representation Specification standard and used within VRS to calculate GA4GH identifiers for high-level domain objects in combination with a type prefix map. The <code>sha512t24u</code> function (Hart et al. 2020) is described as:</p> <ul> <li>performing a SHA-512 digest on a binary blob of data</li> <li>truncate the resulting digest to 24 bytes</li> <li>encodes the 24 bytes using <code>base64url</code> (RFC 4648) resulting in a 32 character string</li> </ul> <p>Under this scheme the string <code>ACGT</code> will result in the <code>sha512t24u</code> digest <code>aKF498dAxcJAqme6QYQ7EZ07-fiw8Kw2</code>. This digest can be converted into a valid refget identifier by prefixing <code>SQ.</code>. </p> <p><code>sha512t24u</code> was envisaged as a fast digest mechanism with a space-efficient representation that can be used for any data with low collision probability. Collisions have been (documented in <code>md5</code>)[https://en.wikipedia.org/wiki/MD5#Collision_vulnerabilities] leading to the belief MD5 was insufficient for our needs.</p> <p><code>sha512t24u</code> must be used for any digest of data by the sequence collections standard. This decision does not disallow the use of <code>md5</code> sequence checksums.</p>"},{"location":"decision_record/#limitations","title":"Limitations","text":"<p><code>MD5</code> is easier to calculate and familiar as many systems ship with a command line <code>md5</code> binary. <code>sha512t24u</code> needs to be typed when used outside of an implementation to avoid issues of collision.</p>"},{"location":"decision_record/#linked-issues_15","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/30</li> </ul>"},{"location":"decision_record/#2023-01-12-how-sequence-collection-are-serialized-prior-to-digestion","title":"2023-01-12 - How sequence collection are serialized prior to digestion","text":"<p>The serialisation in this context is the conversion of the sequence collection object into a string that can be digested.</p>"},{"location":"decision_record/#decision_21","title":"Decision","text":"<p>The serialisation of a sequence collection will use the following steps</p> <ol> <li>Apply RFC-8785 on each array of level 2</li> <li>Digest the canonical representation of each array</li> <li>Create object representation of the seq-col using array names and digested arrays</li> <li>Apply RFC-8785 on the object representation</li> <li>Digest the final canonical representation</li> </ol>"},{"location":"decision_record/#1-apply-rfc-8785-for-converting-from-level-2-to-level-1","title":"1. Apply RFC-8785 for converting from level 2 to level 1","text":"<p>For example the length array at level 2: <pre><code>[248956422, 242193529, 198295559]\n</code></pre></p> <p>Will be serialised using RFC-8785 and digested as a binary string. Here the output of the python implementation: </p> <pre><code>b'[248956422,242193529,198295559]'\n</code></pre> <p>It would also support any UTF-8 character. For example this array of names <pre><code>[\"\u67d3\u8272\u4f53-1\",\"\u67d3\u8272\u4f53-2\",\"\u67d3\u8272\u4f53-3\"]\n</code></pre></p> <p>Would create the following serialisation:</p> <pre><code>b'[\"\\xe6\\x9f\\x93\\xe8\\x89\\xb2\\xe4\\xbd\\x93-1\",\"\\xe6\\x9f\\x93\\xe8\\x89\\xb2\\xe4\\xbd\\x93-2\",\"\\xe6\\x9f\\x93\\xe8\\x89\\xb2\\xe4\\xbd\\x93-3\"]'\n</code></pre>"},{"location":"decision_record/#2-digest-of-the-canonical-representation","title":"2. Digest of the canonical representation","text":"<p>The canonical string representation is then digested. Assuming the use of GA4GH (sha512 trim to 24) digest, the following array of length</p> <pre><code>b'[248956422,242193529,198295559]'\n</code></pre> <p>would be converted to </p> <pre><code>\"5K4odB173rjao1Cnbk5BnvLt9V7aPAa2\"\n</code></pre>"},{"location":"decision_record/#3-creation-of-an-object-composed-of-the-array-names-and-the-digested-arrays","title":"3. Creation of an object composed of the array names and the digested arrays","text":"<p>An object is created with the array name as properties and the digest as value. Example the following collection:  <pre><code>{\n    \"sequences\": \"EiYgJtUfGyad7wf5atL5OG4Fkzohp2qe\",\n    \"lengths\": \"5K4odB173rjao1Cnbk5BnvLt9V7aPAa2\",\n    \"names\": \"g04lKdxiYtG3dOGeUC5AdKEifw65G0Wp\"\n}\n</code></pre></p>"},{"location":"decision_record/#4-use-rfc-8785-on-the-object","title":"4. Use RFC-8785 on the object","text":"<p>This will create a canonical representation of the object </p> <pre><code>b'{\"lengths\":\"5K4odB173rjao1Cnbk5BnvLt9V7aPAa2\",\"names\":\"g04lKdxiYtG3dOGeUC5AdKEifw65G0Wp\",\"sequences\":\"EiYgJtUfGyad7wf5atL5OG4Fkzohp2qe\"}'\n</code></pre>"},{"location":"decision_record/#5-digest-the-final-canonical-representation","title":"5. Digest the final canonical representation","text":"<p>Finally the canonical, representation is digested again to produce the identifier</p> <pre><code>\"S3LCyI788LE6vq89Tc_LojEcsMZRixzP\"\n</code></pre>"},{"location":"decision_record/#rationale_23","title":"Rationale","text":"<p>The decision to use the serialisation of array and object provided in RFC-8785 allows sequence collection to support any type of characters and rely on a documented standard that offer implementation in multiple languages. It also future-proofs the serialisation method if we ever allow complex object to be element of the array.</p>"},{"location":"decision_record/#linked-issues_16","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/1</li> <li>https://github.com/ga4gh/refget/issues/25</li> <li>https://github.com/ga4gh/refget/issues/33</li> </ul>"},{"location":"decision_record/#known-limitations","title":"Known limitations","text":"<p>The JSON canonical serialisation defined in RFC-8785 has a limited set of reference implementation. It is possible that its implementation makes sequence collection implementation more difficult in languages where the RFC is not implemented. In this cases it is valuable to note that the current specification of Sequence Collection do not require that all the features of RFC-8785 be implemented. </p>"},{"location":"decision_record/#alternatives-considered_1","title":"Alternatives considered","text":"<p>We spent a significant amount of time discussing approaches for what essentially amounts to a custom standard for creating the string-to-digest. A lot of this revolved around what delimiters to use. We made a lot of progress there and came up with some really interesting encoding schemas, which had many desirable characteristics. However, ultimately we decided that the value derived from using a comprehensive and well-developed third-party solution would trump the elegance, efficiency, and other benefits we received from our custom encoding schema. In particular, adopting the RFC-8785 would make developers more likely to be able to rely on third-party implementations, reducing the burden to implement our standard. Also, this solution accommodates other sources that we had struggled with a bit, such as UTF-encoding.</p>"},{"location":"decision_record/#2022-10-05-terminology-decisions","title":"2022-10-05 - Terminology decisions","text":""},{"location":"decision_record/#decision_22","title":"Decision","text":"<p>We refer to representations in \"levels\". The level number represents the number of \"lookups\" you'd have to do from the \"top level\" digest. So, we have:</p>"},{"location":"decision_record/#level-0-aka-top-level","title":"Level 0 (AKA \"top level\")","text":"<p>Just a plain digest. This corresponds to 0 database lookups. Example: <pre><code>a6748aa0f6a1e165f871dbed5e54ba62\n</code></pre></p>"},{"location":"decision_record/#level-1","title":"Level 1","text":"<p>What you'd get when you look up the digest with 1 database lookup and no recursion. Previously called \"layer 0\" or \"reclimit 0\" because there's no recursion. Also sometimes called the \"array digests\" because each entity represents an array.</p> <p>Example: <pre><code>{\n  \"lengths\": \"4925cdbd780a71e332d13145141863c1\",\n  \"names\": \"ce04be1226e56f48da55b6c130d45b94\",\n  \"sequences\": \"3b379221b4d6ea26da26cec571e5911c\"\n}\n</code></pre></p>"},{"location":"decision_record/#level-2","title":"Level 2","text":"<p>What you'd get with 2 database lookups (equivalently, 1 recursive call). This is the most common representation, more commonly used than either the \"level 1\" or the \"level 3\" representations.</p> <pre><code>{\n  \"lengths\": [\n    \"1216\",\n    \"970\",\n    \"1788\"\n  ],\n  \"names\": [\n    \"A\",\n    \"B\",\n    \"C\"\n  ],\n  \"sequences\": [\n    \"76f9f3315fa4b831e93c36cd88196480\",\n    \"d5171e863a3d8f832f0559235987b1e5\",\n    \"b9b1baaa7abf206f6b70cf31654172db\"\n  ]\n}\n</code></pre>"},{"location":"decision_record/#level-3","title":"Level 3","text":"<p>What you'd get with 3 database lookups (equivalently, 2 recursive call). The only field that can be further populated is <code>sequences</code>, so the level 3 representation provides the complete data. This layer: - can potentially be very large - is the only level that requires outsourcing a query to a refget server - may reasonable be disabled on my seqcol server, since the point is not to retrieve actual sequences; </p> <p>Example (sequences truncated for brevity): <pre><code>{\n  \"lengths\": [\n    \"1216\",\n    \"970\",\n    \"1788\"\n  ],\n  \"names\": [\n    \"A\",\n    \"B\",\n    \"C\"\n  ],\n  \"sequences\": [\n    \"CATAGAGCAGGTTTGAAACACTCTTTCTGTAGTATCTGCAAGCGGACGTTTCAAGCGCTTTCAGGCGT...\",\n    \"AAGTGGATATTTGGATAGCTTTGAGGATTTCGTTGGAAACGGGATTACATATAAAATCTAGAGAGAAGC...\",\n    \"GCTTGCAGATACTACAGAAAGAGTGTTTCAAACCTGCTCTATGAAAGGGAATGTTCAGTTCTGTGACTT...\"\n  ]\n}\n</code></pre></p>"},{"location":"decision_record/#summary","title":"Summary","text":"<p>We should be consistent by using these terms to refer to the above representation: -  \"level 0 representation\", \"level 0 digest\", top-level digest\", or \"primary digest\"; -  \"level 1 representation\" or \"level 1 digests\"; -  \"level 2 representation\";  -  \"level 3 representation\" of a sequence collection. </p>"},{"location":"decision_record/#linked-issues_17","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/25</li> </ul>"},{"location":"decision_record/#2022-06-15-structure-for-the-return-value-of-the-comparison-api-endpoint","title":"2022-06-15 - Structure for the return value of the comparison API endpoint","text":""},{"location":"decision_record/#decision_23","title":"Decision","text":"<p>The compare function return value MUST be an object following the REQUIRED format specified below.</p> <p>REQUIRED: The endpoint MUST return, in JSON format, an object with these 3 keys: \"digests\", \"arrays\", \"elements\". </p> <ul> <li>digests: an object with 2 elements, with keys a and b, and values either the level 0 seqcol digests for the compared collections, or undefined (null). The value MUST be the level 0 seqcol digest for any digests provided by the user for the comparison. However, it is OPTIONAL for the server to provide digests if the user provided the sequence collection contents, rather than a digest. In this case, the server MAY compute and return the level 0 seqcol digest, or it MAY return undefined (null) in this element for any corresponding sequence collection.</li> <li>arrays: an object with 3 elements, with keys a_only, b_only, and a_and_b. The value of each element is a list of array names corresponding to arrays only present in a, only present in b, or present in both a and b.</li> <li>elements: An object with 3 elements: total, a_and_b, and a_and_b-same-order. total is an object with a and b keys, values corresponding to the total number of elements in the arrays for the corresponding collection. a_and_b is an object with names corresponding to each array present in both collections (in arrays.a_and_b), with values as the number of elements present in both collections for the given array. a_and_b-same-order is also an object with names corresponding to arrays, and the values a boolean following the same-order specification below.</li> </ul> <p>Example: </p> <pre><code>{\n  \"digests\": {\n    \"a\": \"514c871928a74885ce981faa61ccbb1a\",\n    \"b\": \"c345e091cce0b1df78bfc124b03fba1c\"\n  },\n  \"arrays\": {\n    \"a_only\": [],\n    \"b_only\": [],\n    \"a_and_b\": [\n      \"lengths\",\n      \"names\",\n      \"sequences\"\n    ]\n  },\n  \"elements\": {\n    \"total\": {\n      \"a\": 195,\n      \"b\": 25\n    },\n    \"a_and_b\": {\n      \"lengths\": 25,\n      \"names\": 25,\n      \"sequences\": 0\n    },\n    \"a_and_b-same-order\": {\n      \"lengths\": false,\n      \"names\": false,\n      \"sequences\": null\n    }\n  }\n}\n</code></pre>"},{"location":"decision_record/#same-order-specification","title":"Same-order specification","text":"<p>The comparison return includes an a_and_b-same-order boolean value for each array that is present in both collections. The defined value of this attribute is:</p> <ul> <li>undefined (null) if there are fewer than 2 overlapping elements</li> <li>undefined (null) if there are unbalanced duplicates present (see definition below)</li> <li>true if all matching elements are in the same order in the two arrays</li> <li>false otherwise.</li> </ul> <p>An unbalanced duplicate is used in contrast with a balanced duplicate. Balanced means the duplicates are the same in both arrays. When the duplicates are balanced, order is still defined; but if duplicates are unbalanced, this means an array has duplicates not present in the other, and in that case, order is not defined.</p>"},{"location":"decision_record/#rationale_24","title":"Rationale","text":"<p>The primary purpose of the compare function is to provide a high-level view of how two sequence collections match and differ. The primary use cases are to see if collections are identical or subsets, and to assess the degree of overlap in each attribute (such as sharing all sequence digests, sequence names, or lengths). If more details are needed, the user will need to look in more depth at the raw elements of the sequence collection. It's important to have a fast, easy-to-implement, and minimal payload function to provide answers to the common question about \"how compatible are these two collections\".</p>"},{"location":"decision_record/#linked-issues_18","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/21</li> <li>https://github.com/ga4gh/refget/issues/7</li> </ul>"},{"location":"decision_record/#alternatives-considered_2","title":"Alternatives considered","text":"<p>We considered a simpler arrangement that would only return true/false values as to whether the arrays matched but in the different order, or contained any matching elements vs no matching elements. While this would have been faster to compute than the counting approach we settled on, there was concern that it would not be enough information to interpret the comparison. We also considered more information-rich values that would enumerate overlapping or non-overlapping elements. We finally concluded that the most useful would be the middle ground proposed here, where you get counts but no enumerated elements. This provides sufficient information to make a pretty detailed comparison, and can still be computed relatively quickly and keeps the payload size small and predictable.</p>"},{"location":"decision_record/#known-limitations_1","title":"Known limitations","text":"<p>Someone may want to return more information than this, such as enumerating the specific elements in each category. However, this use case would be problematic for large collections, like a transcriptome. We may in the future provide an update to the specification that defines how this information should be returned, but for now, we leave the specification at this minimum requirement.</p>"},{"location":"decision_record/#2022-06-15-we-will-define-the-elements-of-a-sequence-collections-using-a-schema","title":"2022-06-15 - We will define the elements of a sequence collections using a schema","text":""},{"location":"decision_record/#decision_24","title":"Decision","text":"<p>The elements of a sequence collection will be defined and described using JSON Schema. The exact contents of the JSON Schema will be determined later. Here is an example JSON Schema as an illustration of the concept (encoded in YAML for readability):</p> <pre><code>description: \"A collection of sequences, representing biological sequences including nucleotide or amino acid sequences. For example, a sequence collection may represent the set of chromosomes in a reference genome, the set of transcripts in a transcriptome, a collection of reference microbial sequences of a specific gene, or any other set of sequences.\"\ntype: object\nproperties:\n  lengths:\n    type: array\n    description: \"Number of elements, such as nucleotides or amino acids, in each sequence.\"\n    items:\n      type: integer\n  masks:\n    type: array\n    description: \"Digests of subsequence masks indicating subsequences to be excluded from an analysis, such as repeats\"\n    items:\n      type: string\n  names:\n    type: array\n    description: \"Human-readable identifiers of each sequence, commonly called chromosome names.\"\n    items:\n      type: string\n  priorities:\n    type: array\n    description: \"Annotation of whether each sequence is a primary or secondary component in the collection.\"\n    items:\n      type: boolean\n  sequences:\n    type: array\n    description: \"Digests of sequences computed using the GA4GH digest algorithm (sha512t24u).\"\n    items:\n      type: string\n      description: \"Actual sequence content\"\n  topologies:\n    type: array\n    description: \"Annotation of whether each sequence represents a linear or other topology.\"\n    items:\n      type: string\n      enum: [\"circular\", \"linear\"]\n      default: \"linear\"\nrequired:\n  - lengths\ndigested:\n  - lengths\n  - names\n  - masks\n  - priorities\n  - sequences\n  - topologies\n</code></pre>"},{"location":"decision_record/#rationale_25","title":"Rationale","text":"<p>We need a formal definition of a sequence collection. The schema provides a machine-usable definition that specifies the names and types of what we envision as a sequence collection. It also provides a convenient way to describe those elements in human-understandable ways through the <code>description</code> field. Thus, this schema solves a number of issues. Most importantly, it answers the question of what are the elements of the sequence collection, and how are they defined? For example, this schema answers the more specific question: what are the allowable or expected algorithms for items included in the sequence array (or possibly other, custom digested arrays?). The schema establishes a standard expectation for elements that will go into general sequence collections. Implementations are free to add to this schema for their instances as needed.</p>"},{"location":"decision_record/#linked-issues_19","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/8</li> <li>https://github.com/ga4gh/refget/issues/6</li> </ul>"},{"location":"decision_record/#2021-12-01-endpoint-names-and-structure","title":"2021-12-01 - Endpoint names and structure","text":""},{"location":"decision_record/#decision_25","title":"Decision","text":"<p>The endpoint names will be:</p> <ul> <li><code>GET /service-info</code> for GA4GH service info</li> <li><code>GET /collection/{digest}</code> for retrieving a sequence collection</li> <li><code>GET /comparison/{digest1}/{digest2}</code> for comparing two collections in the database</li> <li><code>POST /comparison/{digest1}</code> for comparing one database collection to a local user-provided collection.</li> </ul> <p>The POST body for the local comparison is a \"level 2\" sequence collection, like this:</p> <pre><code>{\n  \"lengths\": [\n    \"248956422\",\n    \"242193529\",\n    \"198295559\"\n  ],\n  \"names\": [\n    \"chr1\",\n    \"chr2\",\n    \"chr3\"\n  ],\n  \"sequences\": [\n    \"a004bc1b0bf05fc668cab6bbfd93d3eb\",\n    \"0ccf3a67666ac53f99fcad19768f2dde\",\n    \"bda7b228789169ae811dd8d676d517ca\"\n  ]\n}\n</code></pre>"},{"location":"decision_record/#rationale_26","title":"Rationale","text":"<p>We wanted to stick with the REST guideline of noun endpoints with GET that describe what you are retrieving. As recommended in the service-info specification, a prefix, like <code>/seqcol/...</code> could be added by a service that implemented multiple specifications, but this kind of namespace it outside the scope of the specification itself. We considered doing <code>/{digest1}/compare/{digest2}</code> and that would have been fine. In the end we liked the symmetry of <code>/comparison</code> and <code>/collection</code> as parallel endpoints. For the retrieval endpoint we considered <code>/seqcol</code> or <code>/sequence-collection</code> or <code>/seqCol</code>, but wanted to keep structure parallel to the refget <code>/sequence</code> endpoint.</p>"},{"location":"decision_record/#limitations_1","title":"Limitations","text":"<p>For the <code>POST comparison</code> endpoint, we made 2 limitations to simplify the implementation of the function. First, we do not require it to allow comparing 2 local collections, which could be enabled, but we reason that users should always be comparing against something in the database, and this prevents abusing the system as a computing engine. We also disallowed (or at least don't explicitly require) comparing a level 1 collection (which consists of a named list of array digests), as we figured that most frequently the user will have the array details, and if not, they could look them up.</p>"},{"location":"decision_record/#linked-issues_20","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/21</li> <li>https://github.com/ga4gh/refget/issues/23</li> </ul>"},{"location":"decision_record/#2021-09-21-order-will-be-recognized-by-digesting-arrays-in-the-given-order-and-unordered-digests-will-be-handled-as-extensions-through-additional-attributes","title":"2021-09-21 - Order will be recognized by digesting arrays in the given order, and unordered digests will be handled as extensions through additional attributes","text":""},{"location":"decision_record/#decision_26","title":"Decision","text":"<p>The final sequence collection digests will reflect the order by digesting the arrays in the order provided. We will employ no additional 'order' array, and no additional unordered digests in the string-to-digest. Any additional attributes designed to handle questions with order, such as <code>sorted_name_length_pairs</code>, will not contribute to the digest. Thus, to determine whether two sequence collections differ only in order will require either 1. using the comparison API; or 2. implementing additional functionality via digests outside the inherent attributes.</p>"},{"location":"decision_record/#rationale_27","title":"Rationale","text":"<p>Our earlier decision determined that order must be reflected in the sequence digests, but did not determine the way to ensure that. After months of debate we came up with 4 competing ideas that could do this:</p> <p>A. Digest arrays in given order. </p> <p>B. Reorder all given arrays according to a single canonical order, and encode order in a separate 'order' array that provides an index into the canonically ordered arrays.</p> <p>C. Reorder each given array individually, and then provide a separate 'order_ATTR' array as an index for each array.</p> <p>D. Store each array in both ordered and unordered form.</p> <p>After lots of initial enthusiasm for option B, we determined that it fails to deliver on the promise of staying invariant when order changes, because if there is a change in any array on which the canonical order is based, this changes the canonical ordering, which in turn changes all the array digests. So these 'unordered' (or canonically ordered) digests are in fact not fit for their main purpose. We therefore agreed to discard this option.</p> <p>While options C/D skirt this issue by having a separate order for each array, so that changes in one array do not affect the digest of another, they add significant complexity as everything needs to be stored twice.</p> <p>To conclude, option A seems simple and straightforward, satisfies for a basic implementation. We thus defer the question of determining whether two sequence collections differ only in order to the comparison API, or to some other future way to do it that will not affect the actual digests (e.g. the 'sorted_name_length_pairs' attribute).</p>"},{"location":"decision_record/#linked-issues_21","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/5</li> </ul>"},{"location":"decision_record/#known-limitations_2","title":"Known limitations","text":"<p>For use cases that require determination of whether two sequence collections differ only in element order, option A will not provide an answer based on digest comparison alone. Instead, the query will be required to use the compatibility API, which means retrieving the contents of the array to compare them.</p> <p>Therefore, to answer this 'order-equivalence' question will require a bit more work than if unordered digests were available; however, this functionality can be easily implemented on top of the basic functionality in a number of ways, which we are continuing to consider.</p>"},{"location":"decision_record/#2021-08-25-sequence-collection-digests-will-reflect-sequence-order","title":"2021-08-25 - Sequence collection digests will reflect sequence order","text":""},{"location":"decision_record/#decision_27","title":"Decision","text":"<p>The final sequence collection digests must reflect the order of the sequences given. In other words, changing the order of the sequences will change the identifier.</p>"},{"location":"decision_record/#rationale_28","title":"Rationale","text":"<p>In some scenarios, the order of the sequences in a collection is irrelevant, and therefore, two collections with identical content but a different order should be considered equivalent. This could lead to an approach of first lexographically sorting input sequences before digesting, so that the final identifier is identical regardless of input order.</p> <p>However, there are also scenarios for which the order of sequences in a collection matters; for example, some aligners output different results based on the input order of the sequences. Or, order may be used to encode sequence priority. Therefore, it is critical that the final identifiers be able to uniquely identify sequence collections with different orders. Because some use cases require order-aware digests, the final algorithm will have to accommodate this, and we will need to come up with another way to identify two collections that are identical in content but with different order, without relying on the digests being identical</p>"},{"location":"decision_record/#linked-issues_22","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/5</li> </ul>"},{"location":"decision_record/#known-limitations_3","title":"Known limitations","text":"<p>This decision will lead to a bit more complexity for use cases that do not care about sequence order, because these use cases will no longer be able to rely on simple, identical digest matching. However, this is a necessary increase in complexity to accommodate other use cases where order matters.</p>"},{"location":"decision_record/#2021-06-30-use-array-based-data-structure-and-multi-tiered-digests","title":"2021-06-30 - Use array-based data structure and multi-tiered digests","text":""},{"location":"decision_record/#decision_28","title":"Decision","text":"<p>Our original formulation structured data with groups of a sequence plus its  annotation, like <code>chr1|248956422|2648ae1bacce4ec4b6cf337dcae37816/chr2|242193529|4bb4f82880a14111eb7327169ffb729b|</code>. Instead, we decided to switch to an array-based model, in which a sequence collection will be constructed as a dictionary object, with attributes as named arrays, like this:</p> <pre><code>seqcol = {'names': ['chrUn_KI270742v1',   'chrUn_GL000216v2',   'chrUn_GL000218v1'],\n  'lengths': ['186739', '176608', '161147'],\n  'sequences': ['2f31c013a4a8301deb8ab7ed1ca1cd99',   '725009a7e3f5b78752b68afa922c090c',   \n'1d708b54644c26c7e01c2dad5426d38c']}\n</code></pre> <p>Digests of this object will be the unique identifier of the sequence collection. To compute the digest of this object, we will use a multi-tiered digest approach, where each array will first be digested separately, and then the final identify will be a digest of digests. A sequence collection may thus be represented as a dictionary of digests:</p> <pre><code>{ 'sequences': '8dd93796fa0225e92eb159a8779f1b254776557f748f8bfb',\n 'lengths': '501fd98e2fdcc276c47306bd72c9155489ed2b23123ddfa2',\n 'names': '7bc90a07cf25f2f64f33baee3d420ad1ae5f442055280d43',\n}\n</code></pre> <p>This will allow retrieving individual attributes, and testing for identity of individual attributes without retrieving more data.</p>"},{"location":"decision_record/#rationale_29","title":"Rationale","text":"<ul> <li>This makes it straightforward to mix-and-match components of the collection. Because each component is independent, and not integrated in with the sequence, it is simpler to select and build subsets and permutations.</li> <li>We can add a new component without eliminating backwards compatibility with previous digests that did not include it, because leaving out an array doesn't change the string to digest.</li> <li>This makes it easier to test for matching sets of sequences, or matching coordinate systems, using the individual component digests. This way we don't have to traverse down a layer deeper, to the individual elements, to establish identity of individual components.</li> <li>It requires only a single delimiter, rather than multiple, which has been a sticking point.</li> </ul>"},{"location":"decision_record/#linked-issues_23","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/8#issuecomment-773489450</li> <li>https://github.com/ga4gh/refget/issues/10</li> </ul>"},{"location":"decision_record/#known-limitations_4","title":"Known limitations","text":"<ul> <li>We may need to enforce that arrays be the same length, at least for attributes that provide one value per sequence. Also, the order of items within each array must match in order for the attributes to correctly collate to a specific sequence.</li> </ul>"},{"location":"decision_record/#2021-01-20-use-the-sam-specification-v1-description-of-sequence-names","title":"2021-01-20 - Use the SAM specification v1 description of sequence names","text":""},{"location":"decision_record/#decision_29","title":"Decision","text":"<p>Sequence collections will use the v1 SAM specification (subsection 1.2.1) to define the allowable characters in a sequence name. As of January 2021, this is:</p> <p>Reference sequence names may contain any printable ASCII characters in the range[!-~]apart from backslashes, commas, quotation marks, and brackets\u2014i.e., apart from \u2018\\ , \"\u2018\u2019 () [] {} &lt;&gt;\u2019\u2014and may notstart with \u2018*\u2019 or \u2018=\u2019.4</p> <p>Should a wider GA4GH standard appear from TASC issue 5 the sequence collection spec will review this decision. The long-term vision of the sequence collections specification is to comply with any eventual GA4GH standard for sequence names.</p>"},{"location":"decision_record/#linked-issues_24","title":"Linked issues","text":"<ul> <li>https://github.com/ga4gh/refget/issues/2</li> </ul>"},{"location":"decision_record/#known-limitations_5","title":"Known limitations","text":"<ul> <li>There have been a handful of reports of old sequences with disallowed characters in the sequence name rows (<code>&gt;</code>) of FASTA files, particularly from the microbiome community. These sequence collections would have to be changed to include only SAM-compatible ASCII characters, which could restrict usage of the sequence collections protocols and delay uptake.</li> </ul>"},{"location":"implementation_api/","title":"Seqcol API service","text":"<p>You can find a demo implementation at https://refget.databio.org.</p> <p>As of 2025, additional implementations are planned or under development at ENA and Ensembl.</p>"},{"location":"implementation_python/","title":"Refget implementation in Python","text":"<p>The reference implementation of refget standards in Python is available here:</p> <ul> <li>Documentation: https://refgenie.org/refget/</li> <li>Source at GitHub: https://github.com/refgenie/refget.</li> <li>PyPI: https://pypi.org/project/refget/</li> </ul>"},{"location":"pangenomes/","title":"Refget pangenomes","text":"<p>TBD</p>"},{"location":"schemas/","title":"Refget schemas","text":"<p>The Refget specifications use schemas to describe data for several purposes. Eventually we intend to provide these via a GA4GH schema registry, once it exists. Until then, the schemas are stored here, and you can access them directly like this:</p> Schema name and link Description seqcol_minimal_v1.0.0.json Minimal schema, describes only the 3 primary attributes (names, lengths, and sequences) seqcol_extended_v1.0.0.json Extended schema, describes all approved attributes seqcol_refs_v1.0.0.json Minimal schema, using references"},{"location":"seqcols/","title":"Refget Sequence Collections v1.0.0","text":""},{"location":"seqcols/#introduction","title":"Introduction","text":"<p>Reference sequences are fundamental to genomic analysis. To make their analysis reproducible and efficient, we require tools that can identify, store, retrieve, and compare reference sequences. The primary goal of the refget Sequence Collections (seqcol) project is to standardize identifiers for collections of sequences. Seqcol can be used to identify genomes, transcriptomes, or proteomes -- anything that can be represented as a collection of sequences. A common example and primary use case of sequence collections is for a reference genome, so this documentation sometimes refers to reference genomes for convenience; really, it can be applied to any collection of sequences.</p> <p>In brief, the project specifies several procedures:</p> <ol> <li>An algorithm for encoding sequence collection identifiers. Refget Sequence Collections extends refget Sequences to collections of sequences. Seqcol also handles sequence attributes, such as their names, lengths, or topologies. Like Refget sequences, seqcol digests are defined by a hash algorithm, rather than an accession authority.</li> <li>An API describing lookup and comparison of sequence collections. Seqcol specifies an HTTP API to retrieve a sequence collection given its digest. This can be used to reproduce the exact sequence collection instead of guessing based on a human-readable identifier. Seqcol also provides a standardized method of comparing the contents of two sequence collections.</li> <li>Recommended ancillary attributes. Finally, the protocol defines several recommended procedures that will improve compatibility across Seqcol servers, and beyond.</li> </ol>"},{"location":"seqcols/#use-cases","title":"Use cases","text":"<p>Sequence collections represent fundamental concepts, making the specification adaptable to a wide range of use cases. A primary goal is to enable sequence collection (seqcol) digests to replace or complement the human-readable identifiers currently used for reference genomes (e.g., \"hg38\" or \"GRCh38\"). Unfortunately, these simple identifiers often refer to references with subtle (or not so subtle) differences. Such variation leads to fundamental issues in analyses relying on reference genomes, undermining the utility of these identifiers.  </p> <p>Unique identifiers, such as those provided by the NCBI Assembly database, partially address this problem by unambiguously identifying specific assemblies. However, this approach has limitations:</p> <ul> <li>It depends on a central authority, which excludes custom genomes and doesn't cover all reference providers.</li> <li>Centralized identifiers alone cannot confirm identity, as identity also depends on the genome's content.  </li> <li>It does not address the related challenge of determining compatibility among reference genomes. Analytical results or annotations based on different references may still be integrable if certain conditions are met, but current tools and standards lack the means to formalize and simplify compatibility comparisons.  </li> </ul> <p>The refget Sequences standard provides a partial solution applicable to individual sequences, such as a single chromosome. However, refget Sequences does not directly address collections of sequences, such as a linear reference genome. Building on refget Sequences, the Sequence Collections specification introduces foundational concepts that support diverse use cases, including:  </p> <ul> <li>Accessing sequences:  As a data analyst, I want to know which sequences are in a specific collection so I can analyze them further. </li> <li>Comparing collections:  As a data analyst, I want to compare the sequence collections used in two separate analyses to assess the compatibility of their resulting data. </li> <li>Annotation curation: As a data curator for SNP data, I want an unambiguous reference genome identifier upon which my SNP annotations can be interpreted, so I can compare them with confidence.</li> <li>Extracting subsets:  As a data analyst, I want to extract specific sequences, such as those composing the chromosomes or karyotype of a genome. </li> <li>Validating submissions: As a submission system, I need to determine the exact content of a sequence collection to validate data file submissions. </li> <li>Embedding identifiers:  As a software developer, I want to embed a sequence collection identifier in my tool's output, allowing downstream tools to identify the exact sequence collection used. </li> <li>Checking compatibility:  As a data analyst using published data, I have a chromosome sizes file (a set of lengths and names) and want to determine whether a given sequence collection is length- or name-compatible with this file. </li> <li>Genome browser integration:  As a genome browser, I use one sequence collection for the displayed coordinate system and want to check if a digest representing a given BED file's coordinate system is compatible with it. </li> <li>Annotating unknown references:  As a data processor, I encounter input data without reference genome information and want to generate a sequence collection digest to attach, enabling further processing with seqcol features. </li> </ul>"},{"location":"seqcols/#architectural-decision-record","title":"Architectural decision record","text":"<p>For a chronological record of decisions related to this specification, see the Architectural Decision Record (ADR).</p>"},{"location":"seqcols/#definitions-of-key-terms","title":"Definitions of key terms","text":""},{"location":"seqcols/#general-terms","title":"General terms","text":"<ul> <li>Alias: A human-readable identifier used to refer to a sequence collection.</li> <li>Array: An ordered list of elements.</li> <li>Coordinate system: An ordered list of named sequence lengths, but without actual sequences.</li> <li>Digest: A string resulting from a cryptographic hash function, such as <code>MD5</code> or <code>SHA512</code>, on input data.</li> <li>Length: The number of characters in a sequence.</li> <li>Level: A way of specifying the completeness of a sequence collection representation. Level 0 is the simplest representation, level 1 more complete, level 2 even more complete, and so forth. Representation levels are described in detail under terminology.</li> <li>Qualifier: A reserved term used in the schema to indicate a quality of an attribute, such as whether it is required, collated, or inherent. Qualifiers are listed below.</li> <li>Seqcol algorithm: The set of instructions used to compute a digest from a sequence collection.</li> <li>Seqcol API: The set of endpoints defined in the retrieval and comparison components of the seqcol protocol.</li> <li>Seqcol digest: A digest for a sequence collection, computed according to the seqcol algorithm.</li> <li>Seqcol protocol: Collectively, the operations outlined in this document, which include: 1. encoding of sequence collections; 2. API describing retrieval and comparison ; and 3. specifications for ancillary recommended attributes.</li> <li>Sequence: Seqcol uses refget sequences to identify actual sequences, so we generally use the term \"sequence\" in the same way. Refget sequences was designed for nucleotide sequences; however, other sequences could be provided via the same mechanism, e.g., cDNA, CDS, mRNA or proteins. Essentially any ordered list of refget-sequences-valid characters qualifies.</li> <li>Sequence digest or refget sequence digest: A digest for a sequence, computed according to the refget sequence protocol.</li> <li>Sequence collection: A representation of 1 or more sequences that is structured according to the sequence collection schema.</li> <li>Sequence collection attribute: A property or feature of a sequence collection (e.g. names, lengths, sequences, or topologies).</li> </ul>"},{"location":"seqcols/#attribute-qualifiers","title":"Attribute qualifiers","text":"<p>These qualifiers apply to a seqcol attribute. These definitions specify something about the attribute if the qualifier is true:</p> <ul> <li>Collated: the values of the attribute match 1-to-1 with the sequences in the collection and are represented in the same order.</li> <li>Inherent: the attribute is part of the definition of the sequence collection and therefore contributes to its digest.</li> <li>Passthru: the attribute is not digested in transition from level 2 to level 1. So its value at level 1 is the same as at level 2.</li> <li>Transient: the attribute cannot be retrieved through the <code>/attribute</code> endpoint.</li> </ul>"},{"location":"seqcols/#seqcol-protocol-functionality","title":"Seqcol protocol functionality","text":"<p>The seqcol algorithm is based on the refget sequence algorithm for individual sequences and should use refget sequence servers to store the actual sequence data. Seqcol servers therefore provide a lightweight organizational layer on top of refget sequence servers. To be fully compliant with the seqcol protocol an implementation must provide all <code>REQUIRED</code> capabilities as detailed below.</p> <p>The seqcol protocol defines the following:</p> <ol> <li>Schema - The way an implementation should define the attributes of sequence collections it holds.</li> <li>Encoding - An algorithm for computing a digest given a sequence collection.</li> <li>API - A server API specification for retrieving and comparing sequence collections.</li> <li>Ancillary attribute management - A specification for organizing non-inherent metadata as part of a sequence collection.</li> </ol>"},{"location":"seqcols/#1-schema-defining-the-attributes-in-the-collection","title":"1. Schema: Defining the attributes in the collection","text":"<p>The first step for a Sequence Collections implementation is to define the list of contents, that is, what attributes are allowed in a collection, and which of these affect the digest. The sequence collections standard is flexible with respect to the schema used, so implementations of the standard can use the standard with different schemas, as required by a particular use case. This divides the choice of content from the choice of algorithm, allowing the algorithm to be consistent even in situations where the content is not. Nevertheless, we RECOMMEND that all implementations start from the same base schema, and add additional attributes as needed, which will not break interoperability. We RECOMMEND not changing the inherent attributes list, because this will keep the identifiers compatible across implementations. Implementations that use different inherent attributes are still compliant with the specification generally, but do so at the cost of top-level digest interoperability. For more information about community-driven updates to the base schema, see Footnote F5.</p> <p>This is the RECOMMENDED minimal base schema:</p> <pre><code>description: \"A collection of biological sequences.\"\ntype: object\nproperties:\n  lengths:\n    type: array\n    collated: true\n    description: \"Number of elements, such as nucleotides or amino acids, in each sequence.\"\n    items:\n      type: integer\n  names:\n    type: array\n    collated: true\n    description: \"Human-readable labels of each sequence (chromosome names).\"\n    items:\n      type: string\n  sequences:\n    type: array\n    collated: true\n    items:\n      type: string\n      description: \"Refget sequences v2 identifiers for sequences.\"\nrequired:\n  - names\n  - lengths\n  - sequences\nga4gh:\n  inherent:\n    - names\n    - sequences\n</code></pre> <p>Sequence collection objects that follow the base minimal structure are said to be the canonical seqcol object representation. The implementation <code>MUST</code> define its structure in a JSON Schema, such as this example. Implementations <code>MAY</code> choose to extend this schema by adding additional attributes. Implementations <code>MAY</code> also use a different schema, but we <code>RECOMMEND</code> the schema extend the base schema defined above.</p> <p>This schema extends vanilla JSON Schema with a few Seqcol-specific attribute qualifiers: the <code>collated</code> and <code>inherent</code> qualifiers.  The specification also defines other attribute qualifiers that are not used in the base schema.  For further details about attribute qualifiers, see Section 4.</p>"},{"location":"seqcols/#2-encoding-computing-sequence-digests-from-sequence-collections","title":"2. Encoding: Computing sequence digests from sequence collections","text":"<p>The encoding function is the algorithm that produces a unique digest for a sequence collection. The input to the function is a set of annotated sequences. This function is generally expected to be provided by local software that operates on a local set of sequences. Example Python code for computing a seqcol digest can be found in the tutorial for computing seqcol digests. For information about the possibilty of deviating from this procedure for custom attributes, see Footnote F6. The steps of the encoding process are described in detail below; briefly, the steps are:</p> <ul> <li>Step 1. Organize the sequence collection data into canonical seqcol object representation and filter the non-inherent attributes.</li> <li>Step 2. Apply RFC-8785 JSON Canonicalization Scheme (JCS) to canonicalize the value associated with each attribute individually.</li> <li>Step 3. Digest each canonicalized attribute value using the GA4GH digest algorithm.</li> <li>Step 4. Apply RFC-8785 JSON Canonicalization Scheme again to canonicalize the JSON of the new seqcol object representation.</li> <li>Step 5. Digest the final canonical representation again using the GA4GH digest algorithm.</li> </ul>"},{"location":"seqcols/#step-1-organize-the-sequence-collection-data-into-canonical-seqcol-object-representation","title":"Step 1: Organize the sequence collection data into canonical seqcol object representation.","text":"<p>We first create an object representation of the attributes of the sequence collection. The structure of this object is critical, and is strictly controlled by the seqcol protocol. The sequence collection object MUST first be structured according to the schema definition for the implementation.</p> <p>Here's an example of a sequence collection organized into the canonical seqcol object representation following the minimal schema example above:</p> <pre><code>{\n  \"lengths\": [\n    248956422,\n    242193529,\n    198295559\n  ],\n  \"names\": [\n    \"chr1\",\n    \"chr2\",\n    \"chr3\"\n  ],\n  \"sequences\": [\n    \"SQ.2YnepKM7OkBoOrKmvHbGqguVfF9amCST\",\n    \"SQ.lwDyBi432Py-7xnAISyQlnlhWDEaBPv2\",\n    \"SQ.Eqk6_SvMMDCc6C-uEfickOUWTatLMDQZ\"\n  ]\n}\n</code></pre> <p>This object would validate against the JSON Schema above. The object is a series of arrays with matching length (<code>3</code>), with the corresponding entries collated such that the first element of each array corresponds to the first element of each other array. For the rationale why this structure was chosen instead of an array of annotated sequences, see Footnote F1.</p>"},{"location":"seqcols/#step-2-apply-rfc-8785-to-canonicalize-the-value-associated-with-each-attribute-individually","title":"Step 2: Apply RFC-8785 to canonicalize the value associated with each attribute individually.","text":"<p>The RFC-8785 JSON Canonicalization Scheme (JCS) standardizes whitespace, character encodings, and other details that would cause inconsequential variations to yield different digests. For most use cases*, the following Python function suffices:</p> <pre><code>import json\n\ndef canonical_str(item: [list, dict]) -&gt; bytes:\n    \"\"\"Convert a list or dict into a canonicalized UTF8-encoded bytestring representation\"\"\"\n    return json.dumps(\n        item, separators=(\",\", \":\"), ensure_ascii=False, allow_nan=False, sort_keys=True\n    ).encode(\"utf8\")\n</code></pre> <p>This will turn the values into canonicalized UTF8-encoded bytestring representations of the list objects. Using Python notation, the value of the lengths attribute becomes <code>b'[248956422,242193529,198295559]'</code>, the value of the names attribute becomes <code>b'[\"chr1\",\"chr2\",\"chr3\"]'</code>, and the value of the sequences attribute becomes </p> <pre><code>b'[\"SQ.2YnepKM7OkBoOrKmvHbGqguVfF9amCST\",\"SQ.lwDyBi432Py-7xnAISyQlnlhWDEaBPv2\",\"SQ.Eqk6_SvMMDCc6C-uEfickOUWTatLMDQZ\"]'\n</code></pre> <p>* The above Python function suffices if (1) attribute keys are restricted to ASCII, (2) there are no floating point values, and (3) for all integer values <code>i</code>:  <code>-2**63 &lt; i &lt; 2**63</code></p> <p>In this process, RFC-8785 is applied only to objects; we assume the sequence digests are computed through an external process (the refget sequences protocol), and are not computed as part of the sequence collection. The refget sequences protocol digests sequence strings without JSON-canonicalization. For more details, see Footnote F2.</p> <p>Exception for passthru attributes</p> <p>This per-attribute digesting procedure (Steps 2 and 3) is applied by default to all attributes of a sequence collection, except for attributes qualified in the schema as passthru; these attributes are NOT digested in this way, but \"passed through\" unchanged to the JSON structure described in Step 3. For more information about passthru attributes, see Section 4.</p>"},{"location":"seqcols/#step-3-digest-each-canonicalized-attribute-value-using-the-ga4gh-digest-algorithm","title":"Step 3: Digest each canonicalized attribute value using the GA4GH digest algorithm.","text":"<p>Apply the GA4GH digest algorithm to each attribute value. The GA4GH digest algorithm is described in detail in Footnote F3. This converts the value of each attribute in the seqcol into a digest string. Applying this to each value will produce the following structure:</p> <pre><code>{\n  \"lengths\": \"5K4odB173rjao1Cnbk5BnvLt9V7aPAa2\",\n  \"names\": \"g04lKdxiYtG3dOGeUC5AdKEifw65G0Wp\",\n  \"sequences\": \"rD29ZKmEqwwHRXjiQ36p6UMZQ5hemmsb\"\n}\n</code></pre> <p>Filter non-inherent attributes</p> <p>The <code>inherent</code> section in the seqcol schema is an extension of the basic JSON Schema format that adds specific functionality. Inherent attributes are those that contribute to the top-level digest; non-inherent attributes are not considered when computing the top-level digest. Attributes of a seqcol that are not listed as <code>inherent</code> <code>MUST NOT</code> contribute to the digest; they are therefore excluded from the top-level digest calculation (Steps 4 and 5). Therefore, if the intermediate seqcol representation includes any non-inherent attributes, these must be removed before proceeding to step 4. In the simple example, the <code>lengths</code> attribute is not <code>inherent</code> and must be filtered.</p>"},{"location":"seqcols/#step-4-apply-rfc-8785-again-to-canonicalize-the-json-of-the-new-seqcol-object-representation","title":"Step 4: Apply RFC-8785 again to canonicalize the JSON of the new seqcol object representation.","text":"<p>Here, we repeat step 2, except instead of applying RFC-8785 to each value separately, we apply it to the entire object. This will result in a canonical bytestring representation of the object, shown here using Python notation:</p> <pre><code>b'{\"names\":\"g04lKdxiYtG3dOGeUC5AdKEifw65G0Wp\",\"sequences\":\"rD29ZKmEqwwHRXjiQ36p6UMZQ5hemmsb\"}'\n</code></pre>"},{"location":"seqcols/#step-5-digest-the-final-canonical-representation-again-using-the-ga4gh-digest-algorithm","title":"Step 5: Digest the final canonical representation again using the GA4GH digest algorithm.","text":"<p>Again using the same approach as in step 3, we now apply the GA4GH digest algorithm to the canonicalized bytestring. The result is the final unique digest for this sequence collection:</p> <pre><code>sjNNwm4zov3Dl0FRWbRTcZwzqrTQKIqL\n</code></pre>"},{"location":"seqcols/#terminology","title":"Terminology","text":"<p>The recursive encoding algorithm leads to several ways to represent a sequence collection. We refer to these representations as \"levels\". The level number represents the number of \"lookups\" you'd have to do from the \"top level\" digest. So, we have:</p>"},{"location":"seqcols/#level-0-top-level-digest","title":"Level 0 (top-level digest)","text":"<p>Just a plain digest, also known as the \"top-level digest\". This corresponds to 0 database lookups. Example: <pre><code>Zjx9_tD2o-1yKB6RR2v2g3W9c5ufydUc\n</code></pre></p>"},{"location":"seqcols/#level-1","title":"Level 1","text":"<p>What you'd get when you look up the digest with 1 database lookup. We sometimes refer to this as the \"attribute digests\", because it is made up a digest for each attribute in the sequence collection. Example:</p> <pre><code>{\n  \"lengths\": \"QWhPI-Cll_0Y5NJ_2krRryuV97vzhbgJ\",\n  \"names\": \"1zOnTYE5slcISev72o62ySxbssEXeoUL\",\n  \"sequences\": \"uPCc00rq-daL3zPnzYH-sBg9_z7HpB8B\"\n}\n</code></pre>"},{"location":"seqcols/#level-2","title":"Level 2","text":"<p>What you'd get with 2 database lookups. This is the most common representation, and hence, it is the level of the canonical seqcol representation. Example:</p> <pre><code>{\n  \"lengths\": [\n    1216,\n    970,\n    1788\n  ],\n  \"names\": [\n    \"A\",\n    \"B\",\n    \"C\"\n  ],\n  \"sequences\": [\n    \"SQ.OL3sVAcd_5IZaDxUkH-yQkLmBz2iwY0s\",\n    \"SQ.kny8cdhEEPHXoNlXmps8NQapGtUKZlM9\",\n    \"SQ.DA-GLdXVihnYKs-fBS5MMgqMi7tVMJbt\"\n  ]\n}\n</code></pre>"},{"location":"seqcols/#3-api-a-server-api-specification-for-retrieving-and-comparing-sequence-collections","title":"3. API: A server API specification for retrieving and comparing sequence collections.","text":"<p>The API has these top-level endpoints:</p> <ol> <li><code>/service-info</code>, for describing information about the service.</li> <li><code>/collection</code>, for retrieving sequence collections.</li> <li><code>/comparison</code>, for comparing two sequence collections.</li> <li><code>/list</code>, for retrieving a list of objects.</li> <li><code>/attribute</code>, for retrieving the value of a specific attribute.</li> </ol> <p>In addition, a RECOMMENDED endpoint at <code>/openapi.json</code> SHOULD provide OpenAPI documentation.</p> <p>Under these umbrella endpoints are a few more specific sub-endpoints, described in detail below:</p>"},{"location":"seqcols/#31-service-info","title":"3.1 Service info","text":"<ul> <li>Endpoint: <code>GET /service-info</code> (<code>REQUIRED</code>)</li> <li>Description: The service info endpoint provides information about the service</li> <li>Return value: Must include the Seqcol JSON Schema that is used by this server</li> </ul> <p>The <code>/service-info</code> endpoint should follow the GA4GH-wide specification for service info for general description of the service. The <code>artifact</code> describing sequence collection entities should be <code>refget.seqcol</code>. See example below.  Then, it should also add a few specific pieces of information under a <code>seqcol</code> property:</p> <ul> <li><code>schema</code>: MUST return the JSON Schema implemented by the server.</li> </ul>"},{"location":"seqcols/#the-service-info-json-schema-document","title":"The service-info JSON-schema document","text":"<p>The <code>schema</code> attribute of <code>service-info</code> return value MUST provide a single schema, this MUST include all possible attributes your service may provide. In other words, you cannot have a collection with an attribute that is not defined in your schema.</p> <p>We RECOMMEND the schema only define terms actually used in at least one collection served; however, it is allowed for the schema to contain extra terms that are not used in any collections in the server.</p> <p>We RECOMMEND your schema use property-level refs to point to terms defined by the minimal base seqcol schema. However, it is also allowed for the schema to embed all definitions locally. The base seqcol schema will be made available as the spec is finalized.</p> <p>For example, here's a JSON schema that uses a <code>ref</code> to reference the approved seqcol schema:</p> <pre><code>description: \"A collection of biological sequences.\"\ntype: object\n\"$id\": \"https://ga4gh.github.io/refget/schemas/seqcol_extended_v1.0.0.json\",  # URI to seqcol schema with ancillary attributes\nproperties:\n  lengths:\n    \"$ref\": \"/lengths\"\n  names:\n    \"$ref\": \"/names\"\n  sequences:\n    \"$ref\": \"/sequences\"\nrequired:\n  - lengths\n  - names\n  - sequences\nga4gh:\n  inherent:\n    - names\n    - sequences\n</code></pre>"},{"location":"seqcols/#example-of-a-service-info-response","title":"Example of a service-info response","text":"<pre><code>{\n  \"id\": \"org.ga4gh.refget.seqcol\",\n  \"name\": \"Sequence Collection server\",\n  \"type\": {\n    \"group\": \"org.ga4gh\",\n    \"artifact\": \"refget-seqcol\",\n    \"version\": \"1.0.0\"\n  },\n  \"description\": \"Collection of sequences from digests.\",\n  \"organization\": {\n    \"name\": \"My organization\",\n    \"url\": \"https://example.com\"\n  },\n  \"contactUrl\": \"mailto:support@example.com\",\n  \"documentationUrl\": \"https://docs.myservice.example.com\",\n  \"createdAt\": \"2024-01-01T11:00:00Z\",\n  \"updatedAt\": \"2024-01-01T11:00:00Z\",\n  \"environment\": \"prod\",\n  \"version\": \"1.4.1\",\n  \"seqcol\": {\n    \"schema\": {\n      \"description\": \"A collection of biological sequences.\",\n      \"type\": \"object\",\n      \"$id\": \"https://ga4gh.github.io/refget/schemas/seqcol_extended_v1.0.0.json\",\n      \"properties\": {\n        \"lengths\": {\n          \"$ref\": \"/lengths\"\n        },\n        \"names\": {\n          \"$ref\": \"/names\"\n        },\n        \"sequences\": {\n          \"$ref\": \"/sequences\"\n        }\n      },\n      \"required\": [ \n        \"names\", \n        \"sequences\" \n      ],\n      \"ga4gh\": {\n        \"inherent\": [\n          \"names\",\n          \"sequences\"\n        ]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"seqcols/#32-collection","title":"3.2 Collection","text":"<ul> <li>Endpoint: <code>GET /collection/:digest?level=:level</code> (<code>REQUIRED</code>)</li> <li>Description: Retrieves original sequence collection from a database, keyed by the unique top-level digest. Here <code>:digest</code> is the seqcol digest computed above. The level corresponds to the \"expansion level\" of the returned sequence collection returned. The default is <code>?level=2</code>, which returns the canonical structure.</li> <li>Return value: The sequence collection identified by the <code>:digest</code> variable. The structure of the data <code>MUST</code> be modulated by the <code>:level</code> query parameter.  Specifying <code>?level=2</code> returns the canonical structure, and <code>?level=1</code> returns the collection with digested attributes.</li> </ul> <p>Non-inherent attributes <code>MUST</code> be stored and returned by the collection endpoint alongside inherent attributes.</p>"},{"location":"seqcols/#33-comparison","title":"3.3 Comparison","text":"<ul> <li>Endpoint 1: <code>GET /comparison/:digest1/:digest2</code> (<code>RECOMMENDED</code>) Two-digest comparison    </li> <li>Endpoint 2: <code>POST /comparison/:digest1</code> (<code>RECOMMENDED</code>) One-digest POST comparison </li> <li>Description: The comparison function specifies an API endpoint that allows a user to compare two sequence collections. The collections are provided either as two digests (the <code>GET</code> endpoint) or as one digest representing a database collection, and one local user-provided collection provided via <code>POST</code>. For the <code>POST</code> endpoint variant, the user-provided local collection should be provided as a level 2 representation (AKA the canonical seqcol representation) in the <code>BODY</code> of the <code>POST</code> request.</li> <li>Return value: The output is an assessment of compatibility between those sequence collections. If implemented, both variants of the <code>/comparison</code> endpoint must <code>MUST</code> return an object in JSON format with these 3 keys: <code>digests</code>, <code>attributes</code>, and <code>array_elements</code>, as described below (see also an example after the descriptions):<ul> <li><code>digests</code>: an object with 2 elements, with keys a and b, and values either the level 0 seqcol digests for the compared collections, or null (undefined). The value MUST be the level 0 seqcol digest for any digests provided by the user for the comparison. However, it is OPTIONAL for the server to provide digests if the user provided the sequence collection contents, rather than a digest. In this case, the server MAY compute and return the level 0 seqcol digest, or it MAY return null (undefined) in this element for any corresponding sequence collection.</li> <li><code>attributes</code>: an object with 3 elements, with keys a_only, b_only, and a_and_b. The value of each element is a list of array names corresponding to arrays only present in a, only present in b, or present in both a and b.</li> <li><code>array_elements</code>: An object with 4 elements: a_count, b_count, a_and_b_count, and a_and_b_same_order. The 3 attributes with _count are objects with keys corresponding to the names of each array present in the collection, or in both collections (for a_and_b_count), with values as the number of elements present either in one collection, or in both collections for the given array. a_and_b_same_order is also an object with keys corresponding to array names, and the values a boolean following the same-order specification below.</li> </ul> </li> </ul> <p>Example <code>/comparison</code> return value:  <pre><code>{\n  \"digests\": {\n    \"a\": \"514c871928a74885ce981faa61ccbb1a\",\n    \"b\": \"c345e091cce0b1df78bfc124b03fba1c\"\n  },\n  \"attributes\": {\n    \"a_only\": [],\n    \"b_only\": [],\n    \"a_and_b\": [\n      \"lengths\",\n      \"names\",\n      \"sequences\"\n    ]\n  },\n  \"array_elements\": {\n    \"a_count\": {\n      \"lengths\": 195,\n      \"names\": 195,\n      \"sequences\": 195\n    },\n    \"b_count\": {\n      \"lengths\": 25,\n      \"names\": 25,\n      \"sequences\": 25\n    },\n    \"a_and_b_count\": {\n      \"lengths\": 25,\n      \"names\": 25,\n      \"sequences\": 0\n    },\n    \"a_and_b_same_order\": {\n      \"lengths\": false,\n      \"names\": false,\n      \"sequences\": null\n    }\n  }\n}\n</code></pre></p>"},{"location":"seqcols/#same-order-specification","title":"Same-order specification","text":"<p>The comparison return includes an a_and_b_same_order boolean value for each array that is present in both collections. The defined value of this attribute is:</p> <ul> <li>undefined (null) if there are fewer than 2 overlapping elements</li> <li>undefined (null) if there are unbalanced duplicates present (see definition below)</li> <li>true if all matching elements are in the same order in the two arrays</li> <li>false otherwise.</li> </ul> <p>An unbalanced duplicate is used in contrast with a balanced duplicate. Balanced means the duplicates are the same in both arrays. When the duplicates are balanced, order is still defined; but if duplicates are unbalanced, this means one array has duplicates not present in the other, and in that case, order is not defined.</p>"},{"location":"seqcols/#interpreting-the-result-of-the-compare-function","title":"Interpreting the result of the compare function","text":"<p>The output of the comparison function provides information-rich feedback about the two collections. These details can be used to make a variety of inferences comparing two collections, but it can take some thought to interpret. For more details about how to interpret the results of the comparison function to determine different types of compatibility, please see the howto guide on comparing sequencing collections.</p>"},{"location":"seqcols/#34-list","title":"3.4 List","text":"<ul> <li>Endpoint: <code>GET /list/:object_type?page=:page&amp;page_size=:page_size&amp;:attribute1=:attribute1_level1_repr&amp;attribute2=:attribute2_level1_repr</code> (<code>REQUIRED</code>)</li> <li>Description: Lists identifiers for a given object type in singular form (e.g. <code>/list/collection</code>). This endpoint provides a way to discover what sequence collections a service provides.   Returned lists can be filtered to only objects with certain attribute values using query parameters.   Page numbering begins at page 0.</li> <li>Return value: The output is a paged list of identifiers following the GA4GH paging guide format, grouped into a <code>results</code> and a <code>pagination</code> section. If no <code>?:attribute=:attribute_level1_repr</code> query parameters are provided, the endpoint will return all items (paged). Adding one <code>:attribute</code> and <code>:attribute_level1_repr</code> pair as query parameters  will filter results to only the collections with the given level 1 attribute digest. If multiple attributes are provided, the filter should require ALL of these attributes to match (so multiple attributes are treated with an <code>AND</code> operator).</li> </ul> <p>Example return value:</p> <pre><code>{\n  \"results\": [\n    ...\n  ],\n  \"pagination\": {\n    \"page\": 1,\n    \"page_size\": 100,\n    \"total\": 165,\n  }\n}\n</code></pre> <p>The <code>list</code> endpoint MUST be implemented, and MUST allow filtering using any attribute defined in the schema, except attributes marked as passthru. For attributes marked as passthru, the list endpoint MAY provide filtering capability.</p>"},{"location":"seqcols/#35-attribute","title":"3.5 Attribute","text":"<ul> <li>Endpoint: <code>GET /attribute/:object_type/:attribute_name/:digest</code> (<code>REQUIRED</code>)</li> <li>Description: Retrieves values of specific attributes in a sequence collection. Here <code>:object_type</code> must be <code>collection</code> for a sequence collection object; <code>:attribute_name</code> is the name of an attribute, such as <code>sequences</code>, <code>names</code>, or <code>sorted_sequences</code>. <code>:digest</code> is the level 1 digest of the attribute value, as computed above.</li> <li>Return value: The attribute value identified by the <code>:digest</code> variable. The structure of the result should correspond to the value of the attribute in the canonical structure.</li> </ul> <p>Example <code>/attribute/collection/lengths/QWhPI-Cll_0Y5NJ_2krRryuV97vzhbgJ</code> return value:</p> <pre><code>[1216, 970, 1788]\n</code></pre> <p>Example <code>/attribute/collection/names/1zOnTYE5slcISev72o62ySxbssEXeoUL</code> return value:</p> <pre><code>[\"A\", \"B\", \"C\"]\n</code></pre> <p>The attribute endpoint MUST be functional for any attribute defined in the schema, except those marked as transient or passthru. The endpoint SHOULD NOT respond to requests for attributes marked as passthru OR transient. For more information on transient and passthru attributes, see Section 4.</p> <p>Definition of <code>object_type</code></p> <p>The <code>/list</code> and <code>/attribute</code> endpoints both use an <code>:object_type</code> path parameter. The <code>object_type</code> should always be the singular descriptor of objects provided by the server. In this version of the Sequence Collections specification, the <code>object_type</code> is always <code>collection</code>; so the only allowable endpoints would be <code>/list/collection</code> and <code>/attribute/collection/:attribute_name/:digest</code>. We call this <code>object_type</code> because future versions of the specification may allow retrieving lists or attributes of other types of objects.</p>"},{"location":"seqcols/#36-openapi-documentation","title":"3.6 OpenAPI documentation","text":"<p>In addition to the primary top-level endpoints, it is RECOMMENDED that the service provide <code>/openapi.json</code>, an OpenAPI-compatible description of the endpoints.</p>"},{"location":"seqcols/#4-extending-the-schema-schema-attribute-qualifiers","title":"4. Extending the schema: Schema attribute qualifiers","text":""},{"location":"seqcols/#41-introduction-to-attribute-qualifiers","title":"4.1 Introduction to attribute qualifiers","text":"<p>The Sequence Collections specification is designed to be extensible. This will let you build additional capability on top of the minimal Sequence Collections standard. You can do this by extending the schema to include ancillary custom attributes. To allow other services to understand something about what these attributes are for, you can annotate them in the schema using  attribute qualifiers. This allows you to indicate what type of attribute your custom attributes are, which govern how the service should respond to requests. This section will describe the 4 attribute qualifiers you may add to the schema to qualify custom attributes.</p>"},{"location":"seqcols/#42-collated-attributes","title":"4.2 Collated attributes","text":"<p>Collated attributes are attributes whose values match 1-to-1 with the sequences in the collection and are represented in the same order. A collated attribute by definition has the same number of elements as the number of sequences in the collection. It is also in the same order as the sequences in the collection.</p>"},{"location":"seqcols/#43-inherent-attributes","title":"4.3 Inherent attributes","text":"<p>Inherent attributes are those that contribute to the digest. The specification in section 1, Encoding, described how to structure a sequence collection and then apply an algorithm to compute a digest for it. What if you have ancillary information that goes with a collection, but shouldn't contribute to the digest? We have found a lot of useful use cases for information that should go along with a seqcol, but should not contribute to the identity of that seqcol. This is a useful construct as it allows us to include information in a collection that does not affect the digest that is computed for that collection. One simple example is the \"author\" or \"uploader\" of a reference sequence; this is useful information to store alongside this collection, but we wouldn't want the same collection with two different authors to have diifferent digests! Seqcol refers to these as non-inherent attributes, meaning they are not part of the core identity of the sequence collection. Non-inherent attributes are defined in the seqcol schema, but excluded from the <code>inherent</code> list. </p> <p>See: ADR on 2023-03-22 regarding inherent attributes</p>"},{"location":"seqcols/#44-passthru-attributes","title":"4.4 Passthru attributes","text":"<p>Passthru attributes have the same representation at level 1 and level 2. In other words, they are not digested in transition from level 2 to level 1. This is not the case for most attributes; most attributes of the canonical (level 2) seqcol representation are digested to create the level 1 representation. But sometimes, we have an attribute for which digesting makes little sense.  These attributes are passed through without transformation, so they show up on the level 1 representation in the same form as the level 2 representation. Thus, we refer to them as passthru attributes.</p> <p>Here's how passthru attributes behave in the endpoints:</p> <ul> <li><code>/list</code>: The server MAY allow filtering on passthru attributes, but this is not required.</li> <li><code>/collection</code>: At both level 1 and level 2, the collection object includes the same passthru attribute representation. </li> <li><code>/comparison</code>: Passthru attributes are listed in the 'attributes' section, but are not listed under 'array_elements'.</li> <li><code>/attribute</code>: Passthru attributes cannot be used with the attribute endpoint, as the return would be the same as the query.</li> </ul>"},{"location":"seqcols/#45-transient-attributes","title":"4.5 Transient attributes","text":"<p>A transient attribute is an attribute that only has a level 1 representation stored in the server. Transient attributes therefore cannot be retrieved through the <code>/attribute</code> endpoint. All other attributes of the sequence collection can be retrieved through the <code>/attribute</code> endpoint. The transient qualifier would apply to attribute that we intend to be used primarily as an identifier. In this case, we don't necessarily want to store the original content that went into the digest into the database. We really just want the final digest. These attributes are called transient because the content of the attribute is no longer stored and is therefore no longer retrievable.</p> <p>Here's how transient attributes behave in the endpoints:</p> <ul> <li><code>/list</code>: No change; a transient attribute level1 representation can be used to list sequence collections that contain it.</li> <li><code>/collection</code>: For level 1 representation, no change; the collection object includes the transient attribute level 1 representation. For level 2 representation, there is a change; transient attributes have no level 2 representation on the server, so the sequence collection SHOULD leave this attribute out of the level 2 representation.</li> <li><code>/comparison</code>: Transient attributes are listed in the 'attributes' section, but are not listed under 'array_elements' because there is no level 2 representation.</li> <li><code>/attribute</code>: Transient attributes cannot be used with the attribute endpoint (there is no value to retrieve)</li> </ul>"},{"location":"seqcols/#46-qualifier-summary-table","title":"4.6 Qualifier summary table","text":"<p>The global qualifiers are all concerned with how the representations are treated when converting between different detail levels. The inherent qualifier is related to the level 1 \u2192 0 transition.  It is true if the level 1 representation is included during creation of level 0 representation. Then the passthru and transient qualifiers are related to the level 2 \u2192 1 transition.</p> Qualifer Level1? Level2? Notes none as normal full content Default state; the level 1 representation is a digest of the level 2 representation. passthru as normal same as level1 True if the level 2 representation is the same as the level 1 representation. transient as normal not present True if the level 2 representation is not present."},{"location":"seqcols/#47-method-of-specifying-attribute-qualifiers","title":"4.7 Method of specifying attribute qualifiers","text":"<p>In JSON Schema, there are 2 ways to qualify properties: 1) a local qualifier, using a key under a property; or 2) an object-level qualifier, which is specified with a keyed list of properties up one level. For example, you annotate a property's <code>type</code> with a local qualifier, underneath the property, like this:</p> <pre><code>properties:\n  names:\n    type: array\n</code></pre> <p>However, you specify that a property is <code>required</code> by adding it to an object-level <code>required</code> list that's parallel to the <code>properties</code> keyword:</p> <pre><code>properties:\n  names:\n    type: array\nrequired:\n  - names\n</code></pre> <p>In sequence collections, we define <code>collated</code> as a local qualifier. Local qualifiers fit better for qualifiers independent of the object as a whole. They are qualities of a property that persist if the property were moved onto a different object. For example, the <code>type</code> of an attribute is consistent, regardless of what object that attribute were defined on. In contrast, object-level qualifier lists fit better for qualifiers that depend on the object as a whole. They are qualities of a property that depend on the object context in which the property is defined. For example, the <code>required</code> modifier is not really meaningful except in the context of the object as a whole. A particular property could be required for one object type, but not for another, and it's really the object that induces the requirement, not the property itself.</p> <p>We reasoned that <code>inherent</code>, <code>transient</code>, and <code>passthru</code> are global qualifiers, like <code>required</code>, which describe the role of an attribute in the context of the whole object. For example, an attribute that is inherent to one type of object need not be inherent to another. Therefore, it makes sense to treat this concept the same way JSON schema treats <code>required</code>. In contrast, the idea of <code>collated</code> describes a property independently: Whether an attribute is collated is part of the definition of the attribute; if the attribute were moved to a different object, it would still be collated.</p> <p>Finally, the 3 global qualiers are grouped under the 'ga4gh' key for consistency with other GA4GH specifications, and to group the seqcol-specific extended functionality into one place.</p>"},{"location":"seqcols/#5-custom-and-recommended-ancillary-attributes","title":"5. Custom and recommended ancillary attributes","text":"<p>The base schema described in Section 1 is a minimal starting point, which can be extended with additional custom attributes as needed. As stated in Section one, we RECOMMEND the schemas extend the base schema with any custom attributes. Furthermore, we RECOMMEND the extended schema add only non-inherent attributes, so that the top-level digests remain compatible. Here, we specify several standard non-inherent attributes that we RECOMMEND be also included in the schema.</p> <p>Furthermore, some attributes do not need to follow the typical encoding process, for whatever reason. Basically, custom attributes can be defined, and they are also allowed to specify their own encoding process.</p>"},{"location":"seqcols/#51-the-name_length_pairs-attribute-recommended","title":"5.1 The <code>name_length_pairs</code> attribute (<code>RECOMMENDED</code>)","text":"<p>The <code>name_length_pairs</code> attribute is a non-inherent attribute of a sequence collection with a formal definition, provided here. This attribute provides a way to look up the ordered coordinate system (the \"chrom sizes\") for a sequence collection. It is created deterministically from the <code>names</code> and <code>lengths</code> attributes in the collection; it does not depend on the actual sequence content, so it is consistent across two collections with different sequence content if they have the same <code>names</code> and <code>lengths</code>, which are correctly collated. This attribute is <code>RECOMMENDED</code> to allow retrieval of the coordinate system for a given reference sequence collection.</p>"},{"location":"seqcols/#algorithm","title":"Algorithm","text":"<ol> <li>Lump together each name-length pair from the primary collated <code>names</code> and <code>lengths</code> into an object, like <code>{\"name\": \"chr1\", \"length\": 123}</code>.</li> <li>Build a collated list, corresponding to the names and lengths of the object (e.g. <code>[{\"name\": \"chr1\", \"length\": 123},{\"name\":\"chr2\", \"length\": 456}],...</code>)</li> <li>Add as a collated attribute to the sequence collection object.</li> </ol>"},{"location":"seqcols/#qualifiers-recommended","title":"Qualifiers (RECOMMENDED)","text":"<ul> <li>inherent: false</li> <li>collated: true</li> <li>passthru: false</li> <li>transient: false</li> </ul>"},{"location":"seqcols/#52-the-sorted_name_length_pairs-attribute-recommended","title":"5.2 The <code>sorted_name_length_pairs</code> attribute (<code>RECOMMENDED</code>)","text":"<p>The <code>sorted_name_length_pairs</code> attribute is similar to the <code>name_length_pairs</code> attribute, but it is sorted. When digested, this attribute provides a digest for an order-invariant coordinate system for a sequence collection. Because it is non-inherent, it does not affect the identity (digest) of the collection.</p> <p>This attribute is <code>RECOMMENDED</code> to allow unified genome browser visualization of data defined on different reference sequence collections. For more rationale and use cases of <code>sorted_name_length_pairs</code>, see Footnote F4.</p>"},{"location":"seqcols/#algorithm_1","title":"Algorithm","text":"<ol> <li>Lump together each name-length pair from the primary collated <code>names</code> and <code>lengths</code> into an object, like <code>{\"name\": \"chr1\", \"length\": 123}</code>.</li> <li>Canonicalize the JSON objects individually according to the seqcol spec (using RFC-8785), e.g. <code>b'{\"length\":123,\"name\":\"chr1\"}'</code> (in Python notation).</li> <li>Digest each name-length pair string individually.</li> <li>Sort the digests lexicographically.</li> <li>Add as an array to the sequence collection object (in level 2). Then, the regular digesting process will digest this array to create the level 1 representation.</li> </ol>"},{"location":"seqcols/#qualifiers-recommended_1","title":"Qualifiers (RECOMMENDED)","text":"<ul> <li>inherent: false</li> <li>collated: false</li> <li>passthru: false</li> <li>transient: true</li> </ul> <p>Transient qualifier for <code>sorted_name_length_pairs</code></p> <p>The <code>sorted_name_length_pairs</code> attribute is recommended as a transient attribute. This is because the level 2 array is mostly an intermediate representation, of little value in itself. If the users are interested in the coordinate system, they can retrieve it from the <code>name_length_pairs</code> attribute. Also, the specific canonical order provided by the algorithm is of little use in other contexts.</p>"},{"location":"seqcols/#53-the-sorted_sequences-attribute-optional","title":"5.3 The <code>sorted_sequences</code> attribute (<code>OPTIONAL</code>)","text":"<p>The <code>sorted_sequences</code> attribute is a non-inherent attribute of a sequence collection, with a formal definition. Providing this attribute is <code>OPTIONAL</code>. When digested, this attribute provides a digest representing an order-invariant set of unnamed sequences. It provides a way to compare two sequence collections to see if their sequence content is identical, but just in a different order. Such a comparison can, of course, be made by the comparison function, so why might you want to include this attribute as well? Simply that for some large-scale use cases, comparing the sequence content without considering order is something that needs to be done repeatedly and for a huge number of collections. In these cases, using the comparison function could be computationally prohibitive. This digest allows the comparison to be pre-computed, and more easily compared.</p>"},{"location":"seqcols/#algorithm_2","title":"Algorithm","text":"<ol> <li>Take the array of the <code>sequences</code> attribute (an array of sequence digests) and sort it lexicographically.</li> <li>Add to the sequence collection object as the <code>sorted_sequences</code> attribute, which is non-inherent and non-collated.</li> </ol>"},{"location":"seqcols/#qualifiers-recommended_2","title":"Qualifiers (RECOMMENDED)","text":"<ul> <li>inherent: false</li> <li>collated: false</li> <li>passthru: false</li> <li>transient: true or false (depending on the use case/implementation)</li> </ul>"},{"location":"seqcols/#footnotes","title":"Footnotes","text":""},{"location":"seqcols/#f1-why-use-an-array-oriented-structure-instead-of-a-sequence-oriented-structure","title":"F1. Why use an array-oriented structure instead of a sequence-oriented structure?","text":"<p>In the canonical seqcol object structure, we first organize the sequence collection into what we called an \"array-oriented\" data structure, which is mainly a list of collated arrays (names, lengths, sequences, etc.). An alternative we considered was a \"sequence-oriented\" structure, which would group each sequence with the other attributes, like <code>{name, length, sequence}</code>, and structure the collection as an array of such objects. While the latter is intuitive, as it captures each sequence object with some accompanying attributes as independent entities, there are several reasons we settled on the array-oriented structure instead: </p> <ol> <li> <p>Flexibility and backwards compatibility of sequence attributes. What happens for an implementation that adds a new attribute? For example, if an implementation adds a <code>topology</code> attribute to the sequences, in the sequence-oriented structure, this would alter the sequence objects and thereby change their digests. In the array-based structure, since we digest the arrays individually, the digests of the other arrays are not changed. Thus, the array-oriented structure emphasizes flexibility of attributes, where the sequence-oriented structure would emphasize flexibility of sequences. In other words, the array-based structure makes it straightforward to mix-and-match attributes of the collection. Because each attribute is independent and not integrated into individual sequence objects, it is simpler to select and build subsets and permutations of attributes. We reasoned that flexibility of attributes was desirable.</p> </li> <li> <p>Conciseness. Sequence collections may be used for tens of thousands or even millions of sequences. For example, a transcriptome may contain a million transcripts. The array-oriented data structure is a more concise representation for digesting collections with many elements because the attribute names are specified once per collection instead of once per element. Furthermore, the level 1 representation of the sequence collection is more concise for large collections, since we only need one digest per attribute, rather than one digest per sequence. </p> </li> <li> <p>Utility of intermediate digests. The array-oriented approach provides useful intermediate digests for each attribute. This digest can be used to test for matching sets of sequences, or matching coordinate systems, using the individual component digests. With a sequence-oriented framework, this would require traversing down a layer deeper, to the individual elements, to establish identity of individual components. The alternative advantage we would have from a sequence-oriented structure would be identifiers for annotated sequences. We can gain the advantages of these digests through adding a custom non-inherent, but collated attribute that calculates a unique digest for each element based on the selected attributes of interest, e.g. <code>named_sequences</code> (digest of e.g. <code>b'{\"name\":\"chr1\",\"sequence\":\"SQ.2648ae1bacce4ec4b6cf337dcae37816\"}'</code>).</p> </li> </ol> <p>See ADR on 2021-06-30 on array-oriented structure</p>"},{"location":"seqcols/#f2-rfc-8785-does-not-apply-to-refget-sequences","title":"F2. RFC-8785 does not apply to refget sequences","text":"<p>A note to clarify potential confusion with RFC-8785. While the sequence collection specification determines that RFC-8785 will be used to canonicalize the JSON before digesting, this is specific to sequence collections, it does not apply to the original refget sequences protocol. According to the sequences protocol, sequences are digested as un-quoted strings. If RFC-8785 were applied at the level of individual sequences, they would be quoted to become valid JSON, which would change the digest. Since the sequences protocol predated the sequence collections protocol, it did not use RFC-8785; and anyway, the sequences are just primitive types so a canonicalization scheme doesn't add anything. This leads to the slight confusion that RFC-8785 canonicalization is only applied to the objects in the sequence collections, and not to the primitives when the underlying sequences are digested. In other words, from the perspective of Sequence Collections, we just take the sequence digests at face value, as handled by a third party; their content is not digested as part of the collections algorithm at a deeper level.</p>"},{"location":"seqcols/#f3-the-ga4gh-digest-algorithm","title":"F3. The GA4GH digest algorithm","text":"<p>The GA4GH digest algorithm, <code>sha512t24u</code>, was created as part of the Variation Representation Specification standard.  This procedure is described as (Hart et al. 2020):</p> <ul> <li>performing a SHA-512 digest on a binary blob of data</li> <li>truncate the resulting digest to 24 bytes</li> <li>encodes the 24 bytes using <code>base64url</code> (RFC 4648) resulting in a 32 character string</li> </ul> <p>In Python, the digest can be computed with this function:</p> <pre><code>import base64\nimport hashlib\n\ndef sha512t24u_digest(seq: bytes) -&gt; str:\n    \"\"\" GA4GH digest function \"\"\"\n    offset = 24\n    digest = hashlib.sha512(seq).digest()\n    tdigest_b64us = base64.urlsafe_b64encode(digest[:offset])\n    return tdigest_b64us.decode(\"ascii\")\n</code></pre> <p>See: ADR from 2023-01-25 on digest algorithm</p>"},{"location":"seqcols/#f4-use-cases-for-the-sorted_name_length_pairs-non-inherent-attribute","title":"F4. Use cases for the <code>sorted_name_length_pairs</code> non-inherent attribute","text":"<p>One motivation for this attribute comes from genome browsers, which may display genomic loci of interest (e.g. BED files). The genome browser should only show BED files if they annotate the same coordinate system as the reference genome. This is looser than strict identity, since we don't really care what the underlying sequence characters are, as long as the positions are comparable. We also don't care about the order of the sequences. Instead, we need them to match level 1 digest of the <code>sorted_name_length_pairs</code> attribute. Thus, to assert that a BED file can be viewed for a particular genome, we compare the <code>sorted_name_length_pairs</code> digest of our reference genome with the sequence collection used to generate the file. There are only two possibilities for compatibility: 1) If the digests are equal, then the data file is directly compatible; 2) If not, we must check the <code>comparison</code> endpoint to see whether the <code>name_length_pairs</code> attribute of the sequence collection is a direct subset of the same array in the sequence collection attached to the genome browser instance. If so, the data file is still compatible. </p> <p>For efficiency, if the second case is true, we may cache the <code>sorted_name_length_pairs</code> digest in a list of known compatible reference genomes. In practice, this list will be short. Thus, in a production setting, the full compatibility check can be reduced to a lookup into a short, pre-generated list of <code>sorted_name_length_pairs</code> digests.</p> <p>See: ADR from 2023-07-12 on sorted name-length pairs</p>"},{"location":"seqcols/#f5-adding-new-schema-attributes","title":"F5. Adding new schema attributes","text":"<p>A strength of the seqcol standard is that the schema definition can be modified for particular use cases, for example, by adding new attributes into a sequence collection. This will allow different communities to use the standard without necessarily needing to subscribe to identical schemas, allowing the standard to be more generally useful. However, if communities define too many custom attributes, this leads to the possibility of fragmentation. For example, two implementations may start using the same attribute name to refer to different concepts. While the attributes would still be formally defined in the respective schemas provided by each implementation, calling different concepts by the same name would come at the cost of interoperability. Therefore, the standard will also include in the schema a list of formally defined attributes, to encourage interoperability of these attributes. The goal is not to include all possible attributes in the schema, but just those likely to be used repeatedly, to encourage interoperable use of those attribute names.</p> <p>An implementation may propose a new attribute to be added to this extended schema by raising an issue on the GitHub repository. The proposed attributes and definition can then be approved through discussion during the refget working group calls and ultimately added to the approved extended seqcol schema. These GitHub issues should be created with the label 'schema-term'. You can follow these issues (or raise your own) at https://github.com/ga4gh/seqcol-spec/issues?q=is%3Aissue+label%3Aschema-term+.</p>"},{"location":"seqcols/#f6-custom-encoding-algorithms","title":"F6. Custom encoding algorithms","text":"<p>A core part of Sequence Collections specification is the encoding algorithm, which describes how to create the digest for a sequence collection. The encoding process can be divided into two steps; first, the attributes are encoded into the level 1 representation, and then this is encoded to produce the final digest (also called the level 0 or top level representation). The first part of this process, encoding from level 2 to level 1, is the default; this is applied to any attributes that don't have something else defined specifically as part of the attribute definition. This is the way all the minimal attributes (names, lengths, and sequences) should behave. But custom attributes MAY diverge from this approach by defining their own encoding procedure that defines how the level 1 digest is computed from the level 2 representation. For example, in the list of recommended ancillary attributes, <code>name_length_pairs</code> does not define a custom procedure for encoding, so this would follow the default procedure. An alternative custom attribute, though, MAY specify how this encoding procedure happens, as for example, the recommended <code>sorted_name_length_pairs</code> attribute does.</p>"},{"location":"seqcols/compare_collections/","title":"How to: Compare two collections","text":""},{"location":"seqcols/compare_collections/#use-case","title":"Use case","text":"<ul> <li>You have two digests for collections you know are stored by a server. You want to compare them.</li> <li>You have a digest for a collection from a server, and a local sequence. You want to compare the two to see if they have the same coordinate system.</li> </ul>"},{"location":"seqcols/compare_collections/#how-to-do-it","title":"How to do it","text":"<p>You can use the <code>/comparison/:digest1/:digest2</code> endpoint to compare two collections. You can also <code>POST</code> a local collection to <code>/comparison/:digest1</code> to compare it to a single remote collection. The comparison function gives information-rich feedback about the two collections, but it can take some thought to interpret. Here are some examples.</p> <p>The best way is to use the Refget Seqcol Comparison Interpretation Module (SCIM). You paste in the JSON output of a comparison, and it provides an interpretation for you.</p>"},{"location":"seqcols/compare_collections/#interpretation-details","title":"Interpretation details","text":""},{"location":"seqcols/compare_collections/#strict-identity","title":"Strict identity","text":"<p>Some analyses may require that the collections be strictly identical -- that is, they have the same sequence content, with the same names, in the same order. For example, aligning with bowtie2 against sequence collections that differ in either content, name, or order will not necessarily produce the same output. Therefore, we must be able to identify that two sequence collections are identical in terms of sequence content, sequence name, and sequence order. </p> <p>For this simple comparison, you don't need the <code>/comparison</code> endpoint -- just compare the top-level digests. Two collections will have the same digest if they are identical in content, names, and order for all <code>inherent</code> attributes. Therefore, if the digests differ, then you know the collections differ in at least one inherent attribute.</p>"},{"location":"seqcols/compare_collections/#order-relaxed-identity","title":"Order-relaxed identity","text":"<p>A process that treats each sequence independently and re-orders its results will return identical results as long as the sequence content and names are identical, even if the order doesn\u2019t match. Therefore, you may be interested in saying \"these two sequence collections have identical sequence names and content, but differ in order\". Relying on top-level digests will not work for this comparison, but you can answer this question using <code>/comparison</code> return value:</p> <p>Two collections meet the criteria for order-relaxed identity if:</p> <ol> <li>the value of the <code>elements.total.a</code> and <code>elements.total.b</code> match, (the collections have the same number of elements).</li> <li>this value is the same as <code>elements.a_and_b.&lt;attribute&gt;</code> for all attributes (the content is the same)</li> <li>any entries in <code>elements.a_and_b-same-order.&lt;attribute&gt;</code> may be true (indicating the order matches) or false (indicating the order differs)</li> </ol> <p>Then, we know the sequence content and names are identical, but not in the same order. </p>"},{"location":"seqcols/compare_collections/#name-relaxed-identity","title":"Name-relaxed identity","text":"<p>Some analysis (for example, a <code>salmon</code> RNA pseudo-alignment) will be identical regardless of the chromosome names, as it considers the digest of the sequence only. Thus, we'd like to be able to say \"These sequence collections have identical content, even if their names and/or orders differ.\"</p> <p>There are two convenient ways to answer this question. First, you can use the attribute (level1) digest, for the <code>sorted_sequences</code> attribute. If this digest matches, then you know you have identical sequence content, without controlling for names or sequence order.</p> <p>Second, you can also answer this question using the <code>/comparison</code> function. As long as the <code>a_and_b</code> number for <code>sequences</code> equals the values listed in <code>elements.total</code>, then the sequence content in the two collections is identical.</p>"},{"location":"seqcols/compare_collections/#length-only-compatible-shared-coordinate-system","title":"Length-only compatible (shared coordinate system)","text":"<p>A much looser type of compatibility is two sequence collections that have the same set of sequence lengths, though the sequences themselves may differ. In this case we may or may not require name identity. For example, a set of ATAC-seq peaks that are annotated on a particular genome could be used in a separate process that had been aligned to a different genome, with different sequences -- as long as the lengths and names were shared between the two analyses.</p> <p>How to assess: We will ignore the <code>sequences</code> attribute, but ensure that the <code>names</code> and <code>lengths</code> numbers for <code>a_and_b</code> match what we expect from <code>elements.total</code>. If the <code>a_and_b-same-order</code> is also true for both <code>names</code> and <code>lengths</code>, then we can be assured that the two collections share an ordered coordinate system. If however, their coordinate system matches but is not in the same order, then we require looking at the <code>sorted_name_length_pairs</code> attribute. If the <code>a_and_b</code> entry for <code>sorted_name_length_pairs</code> is the same as the number for <code>names</code> and <code>lengths</code>, then these collections share an (unordered) coordinate system.</p>"},{"location":"seqcols/compare_collections/#complex-cases","title":"Complex cases","text":"<p>For more complex cases, the comparison function and the level1 digests can sometimes be used to figure out what is going on, but they are limited by design -- for situations that are more complex than these methods can handle, it is always possible to look deeper at the contents of the sequence collection and compare them directly. </p> <p>The <code>/comparison</code> endpoint only tests the order of each array attribute independently. There is no general test of order consistency across several array attributes, e.g. whether a single set of collated values for names, lengths, and sequences retains the same index across all three arrays if reordered. A concrete example for interpreting such a case will be added later.</p>"},{"location":"seqcols/digest_from_collection/","title":"How to: Compute a seqcol digest given a sequence collection","text":""},{"location":"seqcols/digest_from_collection/#use-case","title":"Use case","text":"<p>One of the most common uses of the seqcol specification is to compute a standard, universal digest for a particular sequence collection. You have a collection of sequences, like a reference genome or transcriptome, and you want to determine its seqcol digest. There are two ways to approach this: 1. Using an existing implementation; 2. Implement the seqcol digest algorithm yourself (it's not that hard).</p>"},{"location":"seqcols/digest_from_collection/#1-using-existing-implementations","title":"1. Using existing implementations","text":"<ul> <li>The <code>refget</code> Python package provides an implementation for local computation of digests: https://refgenie.org/refget/.</li> </ul>"},{"location":"seqcols/digest_from_collection/#2-implement-the-seqcol-digest-algorithm-yourself","title":"2. Implement the seqcol digest algorithm yourself","text":"<p>Follow the procedure under the section for Encoding. Briefly, the steps are:</p> <ul> <li>Step 1. Organize the sequence collection data into canonical seqcol object representation.</li> <li>Step 2. Apply RFC-8785 JSON Canonicalization Scheme (JCS) to canonicalize the value associated with each attribute individually.</li> <li>Step 3. Digest each canonicalized attribute value using the GA4GH digest algorithm.</li> <li>Step 4. Apply RFC-8785 JSON Canonicalization Scheme again to canonicalize the JSON of new seqcol object representation.</li> <li>Step 5. Digest the final canonical representation again.</li> </ul> <p>Details on each step can be found in the specification.</p>"},{"location":"seqcols/digest_from_collection/#example-python-code-for-computing-a-seqcol-encoding","title":"Example Python code for computing a seqcol encoding","text":"<pre><code># Demo for encoding a sequence collection\n\nimport base64\nimport hashlib\nimport json\n\ndef canonical_str(item: dict) -&gt; bytes:\n    \"\"\"Convert a dict into a canonical string representation\"\"\"\n    return json.dumps(\n        item, separators=(\",\", \":\"), ensure_ascii=False, allow_nan=False, sort_keys=True\n    ).encode(\"utf8\")\n\ndef sha512t24u_digest(seq: bytes) -&gt; str:\n    \"\"\" GA4GH digest function \"\"\"\n    offset = 24\n    digest = hashlib.sha512(seq).digest()\n    tdigest_b64us = base64.urlsafe_b64encode(digest[:offset])\n    return tdigest_b64us.decode(\"ascii\")\n\n# 1. Get data as canonical seqcol object representation\n\nseqcol_obj = {\n  \"lengths\": [\n    248956422,\n    133797422,\n    135086622\n  ],\n  \"names\": [\n    \"chr1\",\n    \"chr2\",\n    \"chr3\"\n  ],\n  \"sequences\": [\n    \"SQ.2648ae1bacce4ec4b6cf337dcae37816\",\n    \"SQ.907112d17fcb73bcab1ed1c72b97ce68\",\n    \"SQ.1511375dc2dd1b633af8cf439ae90cec\"\n  ]\n}\n\n# Step 1a: Remove any non-inherent attributes,\n# so that only the inherent attributes contribute to the digest.\n# Only names and sequences are inherent.\n\nseqcol_obj_inherent = {\n  \"names\": seqcol_obj[\"names\"],\n  \"sequences\": seqcol_obj[\"sequences\"]\n}\n\n# Step 2: Apply RFC-8785 to canonicalize the value \n# associated with each attribute individually.\n\nseqcol_obj2 = {}\nfor attribute in seqcol_obj_inherent:\n    seqcol_obj2[attribute] = canonical_str(seqcol_obj_inherent[attribute])\nseqcol_obj2  # visualize the result\n\n# Step 3: Digest each canonicalized attribute value\n# using the GA4GH digest algorithm.\n\nseqcol_obj3 = {}\nfor attribute in seqcol_obj2:\n    seqcol_obj3[attribute] = sha512t24u_digest(seqcol_obj2[attribute])\nprint(json.dumps(seqcol_obj3, indent=2))  # visualize the result\n\n# Step 4: Apply RFC-8785 again to canonicalize the JSON \n# of new seqcol object representation.\n\nseqcol_obj4 = canonical_str(seqcol_obj3)\nseqcol_obj4  # visualize the result\n\n# Step 5: Digest the final canonical representation again.\n\nseqcol_digest = sha512t24u_digest(seqcol_obj4)\n# Answer: 'KxZO6qIbVNCIKtQj0WR3fwzg2rsJLlC3'\n\n# Equivalent to:\n# import refget\n# sc = refget.SequenceCollection.from_dict(seqcol_obj)\n# sc.digest\n</code></pre>"},{"location":"seqcols/recipes/","title":"Recipes","text":""},{"location":"seqcols/recipes/#operations-enabled-by-seqcol-organized-by-input","title":"Operations enabled by seqcol, organized by input","text":""},{"location":"seqcols/recipes/#seqcol-digest-as-input","title":"Seqcol digest as input","text":"<ul> <li>seqcol digest -&gt; sequence digests: Given a seqcol digest, retrieve sequence digests for all contained sequences with the <code>/collection/:digest/:level</code> endpoint by setting the <code>level</code> to 1, or omitting it.</li> <li>seqcol digest -&gt; sequences: Given a seqcol digest, sequences themselves for all contained sequences can be retrieved by the <code>/collection/:digest/:level</code> endpoint by setting the <code>level</code> to 2 (if this is allowed by the server). Or, you can use the sequence digests retrieve at <code>level=1</code> to look up actual sequences using a refget server.</li> <li>seqcol digest -&gt; metadata: Any metadata known by the server will be retrieved by using <code>/collection/:digest</code>.</li> <li>seqcol digest -&gt; aliases of seqcol: Aliases are a not a built-in part of the seqcol spec, so this capability will depend on the underlying provider. If the provider provides aliases, you can retrieve them using <code>/collection/:digest</code>.</li> <li>2 seqcol digests -&gt; assessment of compatibility: Provided by the <code>/comparison</code> endpoint.</li> <li>seqcol digest + sequences -&gt; validation claim: Use the seqcol algorithm to compute the digest of the set of sequences, and then use the comparison function to validate (or, if you validation requires strict identity, just confirm that the digests match).</li> </ul>"},{"location":"seqcols/recipes/#sequence-collection-as-input","title":"Sequence collection as input","text":"<ul> <li>sequences -&gt; refget digests: Individual sequences can be converted into refget digests using the refget algorithm</li> <li>sequences -&gt; seqcol digest: Sequence collections can be converted into seqcol digests using the seqcol algorithm. To obtain a sequence digest, follow the algorithm outlined above, sequences -&gt; refget digests -&gt; seqcol digest.</li> </ul>"},{"location":"seqcols/recipes/#refget-digest-as-input","title":"Refget digest as input","text":"<ul> <li>refget digest -&gt; seqcol digests containing this sequence: Use the <code>containing_collections</code> secondary endpoint. </li> <li>many refget digests -&gt; seqcol digests containing all of these sequences: Repeated application of <code>containing_collections</code> endpoint?</li> </ul>"},{"location":"seqcols/recipes/#coordinate-system-as-input","title":"Coordinate system as input","text":"<ul> <li>coordinate system -&gt; seqcol digest: If you have only a coordinate system (lengths with no sequences), you may still compute a seqcol digest using the seqcol algorithm, because the sequences themselves are optional. Coordinate systems can therefore be uniquely encoded, retrieved, and compared using the seqcol protocol.</li> </ul>"},{"location":"seqcols/recipes/#related-operations-what-seqcol-does-not-do","title":"Related operations: What seqcol does not do","text":"<ul> <li>sequences -&gt; sequence names: This is the refget reverse lookup.</li> <li>alias -&gt; seqcol digest: Will the API offer this?</li> <li>seqcol digest -&gt; other seqcol digests compatible at some level: The database does not store comparison flags, so if you're searching for compatible collections you'd need to retrieve and compute the compatible yourself using the comparison function.</li> </ul>"},{"location":"seqcols/seqcol_rationale/","title":"Seqcol rationale","text":""},{"location":"seqcols/seqcol_rationale/#introduction","title":"Introduction","text":"<p>The Refget Sequence Collections standard specification is written mostly to answer the question of how to implement the standard, rather than explaining why we designed the standard the way we did. But because there are many different use cases and perspectives with an interest in sequence collections, several readers of the specification have completed reading it and understand how to do it, but are confused about why we made some of the decisions we did, particularly because the specification doesn\u2019t perfectly fit with their expectations of the easiest way to solve their specific use case. This document is my attempt to broaden the perspective, provide the rationale for why on some of the more controversial decisions, and hopefully convince readers that the years of thought and discussion that went into sequence collections was useful and created a standard that, although not necessarily the easiest or simplest way to solve one particular use case, is an elegant balance between complexity and simplicity that allows it to solve a huge number of related use cases while maintaining interoperability wherever possible and flexibility when truly needed.</p> <p>More specifically, this document attempts to answer these questions:</p> <ul> <li>Why should the specification include a specific recommended schema, with a defined set of terms, including which are required (must exist) and which are inherent (affect the outcome of the digest)?</li> <li>Why is there not a digest for <code>&lt;insert my use case here&gt;</code>, so I can just compare two strings to see if the sequence collections are the same? Why do you need these more complex digests that make my comparison harder?</li> <li>What is the purpose of the comparison function, which seems more complicated than a simple string match?</li> <li>Why do you organize the sequence collection as a set of arrays, instead of an array of annotated sequence objects?</li> <li>Why do you do repeated layers of digests, instead of just building one string for the collection and digesting it, which seems simpler?</li> <li>Why is it important to include names, and lengths, instead of just the sequences themselves?</li> </ul>"},{"location":"seqcols/seqcol_rationale/#a-tale-of-three-use-cases","title":"A tale of three use cases","text":"<p>A major need that drove seqcol development is the need to see if two sequence collections match. This need is repeated across many use cases, and a main motivating factor in choosing a deterministic, content-derived identifier, because it lets you know if the content is identical by simply asking whether the digests are identical. Early on in our discussions of sequence collection use cases, it was clear that different use cases would have slightly different formulations of what is important in a sequence collection. For example, a first use case is that of the archiver: we just need to assess whether the content of the sequences in two sets are the same. The archiver doesn't care about the order of the sequences, or the names of the sequences, or any other metadata attributes. In this case, it would be simple to just digest each sequence, sort the digests, concatenate them, and then digest that string -- and in fact, this was an early simple proposal for the sequence collection standard. But while this would work perfectly fine for the simple use case of asserting two sets of sequences match in content, this is not the only use case for sequence collections.</p> <p>Another use case is that of the aligner: say we want to reproduce a computational pipeline that does sequence alignment. We need an identifier of the reference genome (sequence collection) used for the alignment, so that when we repeat it, we guarantee we are using the same reference. In this scenario, the sorted, concatenated sequence digests wouldn't work for two reasons: First, for many aligners, the order of the sequences matter. Therefore, the final digest representing the collection should differ if the underlying sequences had different order. Second, the output of the aligner is going to specify the location of each read with respect to the name of a sequence. A sequence collection with the same sequences, even in the same order, would yield a different alignment output file if the sequences have different names. Therefore, the final digest should differ if the names of the sequences do not match. To generate a digest useful for this use case, we could adjust the prior proposal by 1) not sorting the sequence digests prior to concatenation; and 2) adding in a names vector. Digesting this would yield a content-derived identifier that would match only if the analysis could be reproduced entirely. Thus, already we have two use cases with different immediate answers to question; in the first, we imagine an identifier computed from sorted sequence digests, and in the second, we imagine an identifier computed from sequences plus their names in their existing order.</p> <p>Now, here's yet another use case not served by either of these identifiers: the analyst. An analyst, further downstream, uses data aligned to a reference genome and summarized into genomic annotations. Say they have ChIP-seq data, which defines the binding locations of various transcription factors stored in BED file format. These datasets are columns of a sequence name, plus coordinate start and end for regions of interest. More than 80,000 BED files have been posted on GEO, as these results vary by cell-type, treatment, species, age, etc. The user now is interested in integrating BED files from different studies, either to visualize or analyze them together -- but it only makes sense to integrate them if the BED coordinates are defined on the same coordinate system. If defined on different coordinate systems, a position in one does not correspond to a position in another, and they cannot be easily integrated; additional processing would be required, depending on how different the reference genomes are. We would like the digest to somehow inform us as to whether the sequence collections are compatible at the level of a coordinate system. In this use case, the underlying sequence content is irrelevant, as long as the coordinate systems match. Therefore, a digest should consider the names and lengths of the sequences, but not the actual sequence content. This leads to a third vision of a sequence collection but where the actual base pairs don't matter; what's important is the names and lengths of those sequences.</p>"},{"location":"seqcols/seqcol_rationale/#answering-the-question","title":"Answering the question","text":"<p>It's clear from these examples that we have multiple common and reasonable use cases that use collections of sequences in different ways and therefore lead to a different optimal identifier. How could a standard accommodate all these different viewpoints?</p> <p>An immediate solution is to let each use case define its own identifier. That would solve each problem individually; and yet, it is unsatisfying because these resulting identifiers would not be interoperable. The way the archiver computes the identifier wouldn't be useful for the aligner; the aligner's identifier wouldn't be useful for the epigenome analyst, and so on. So these one-off digests would not be able to become the universal identifiers we envisioned at the outset.  We wondered if there could be a way to solve all these different needs, but in a way that maintained some interoperability so the identifiers could somehow be used together within a common ecosystem. Clearly, some aspects of the use cases are shared -- could we use this to do better than the individual solutions that neglect interoperability?</p> <p>To do this, we employed several strategies in the sequence collections standard:</p> <ul> <li>First, we use a schema to separate the question of \"what gets included\" from the rest of the standard.</li> <li>Second, we deploy the comparison function, a more powerful way of comparing two sequence collections.</li> <li>Third, we specify a layered algorithm for computing digests, where individual attributes are digested separately and then these are digested again to make the final digest. This provides intermediate digests that can be used for different purposes and also makes it easy to define a custom internal digests.</li> </ul> <p>These strategies don\u2019t solve all the problems individually, but taken together, they allowed us to design an elegant, powerful, and extensible structure that provides reasonably easy solutions to all our use cases, while maintaining a degree of interoperability among identifiers. Let's dive into each of these strategies in detail to see how it helps us get the best of both worlds.</p>"},{"location":"seqcols/seqcol_rationale/#1-handling-divergent-needs-by-splitting-the-standard-into-two-parts","title":"1. Handling divergent needs by splitting the standard into two parts","text":"<p>Our first strategy splits the identifier definition into two parts: 1) the algorithm by which the identifier will be computed, and 2) the list of what content contributes to the digest. The algorithm means the sorting approach, hash function, delimiters, concatenation process, etc. -- how to construct the string that gets digested to make the identifier. The second part is the list of content, which asks what attributes affect the digest; that is, do we include the names, sequences, etc in the string to digest? The division is useful because many of the differences come from different choice of content, not from a different algorithm, meaning the algorithm could be consistent even in situations where the content is not. For example, the second use case requires sequences, but the third use case does not. The algorithm can be kept the same.</p> <p>Separating these two tasks also has a conceptual benefit: it isolates the critical question: exactly which attributes of a sequence collection should contribute to the digest computation? The standard abstracts away this question by allowing an implementation to specify which attributes contribute to the digest through the \"inherent\" property in the schema. This lets us publish a proposal for the algorithm, but leave the final decision open to the community (or even to different versions to be used by different communities). So people can use the sequence collections standard with whatever schema they want, providing flexibility to handle a wide variety of use cases by changing which attributes are listed as inherent in the schema.</p> <p>The split into two parts thus provides some important modularity, but it doesn't solve all the issues. Two important challenges remain: First, it does not provide flexibility to adjust whether the content is sorted, since that's part of the algorithm, not part of the schema. Since this also differs between the first two use cases (the archiver use case wants to sort the sequence digests, but the aligner use case cannot), it means just splitting out the list of inherent attributes is insufficient for accommodating all the use cases. The other, more important issue that remains unsolved is that the identifiers computed with different schemas will, by definition, be different -- meaning we haven't solved the interoperability challenge. Thus, our first strategy helps solve some of the needs to be applied to different use cases, but it doesn't do enough to maintain interoperability.</p>"},{"location":"seqcols/seqcol_rationale/#2-the-comparison-function","title":"2. The comparison function","text":"<p>The second strategy is the comparison function. In short, the goal here is to move away from comparing collections by simply checking if their digests are identical; instead, we want a more powerful comparison that can answer multiple questions using a single digest.</p> <p>In more detail: each of the scenarios described above can be viewed as constructing a digest to make it really easy to ask a particular comparison question. For example, in the first use case, the question is \"do these two collections have exactly the same sequence content, regardless of order?\" In the second example, the question is more general: \"do these two collections have exactly the same sequence content and sequence names, in the same order?\". The third question is \"Do these collections have the same coordinate system, in the same order?\". To answer any of these questions, if you had the bespoke digest, you'd simply see if two digests are identical. If they are, the comparison question is satisfied. This is convenient if your question happens to be the one used to construct the digest. But the problem is that the final digest representing a sequence collection can only answer one such question. A simple string check approach simply offers only a single comparison: are these two things identical, or not? Therefore, to accommodate our complex use cases, where we have multiple things we want to compare in different scenarios, we need something more than just comparing digests. We need a single digest to be able to answer all of the above questions, and more.</p> <p>Here's our alternative: instead of a digest-matching query, we design a comparison function. The comparison function goes beyond simply comparing digest strings; it provides a comparison of every attribute in the collection, including how many elements match, whether they are in the same order, and more. The output lets you answer all the questions posed above, plus more. You can tell if two collections have the same sequences, whether they are in the same order, whether their names match, whether the sequences differ but the lengths match, etc. It also allows you to determine more complex comparisons: is one sequence collection a subset of another? Do they have at least some sequences or names in common? Are their coordinate systems compatible, even if the sequence content differs? All of these questions are immediately answerable from the result of the comparison function.</p> <p>To give a specific example of why the comparison function is better than relying on digest matching, let's revisit the first and second use cases. Whereas the original digest matching approach would require a separate identifier for each use case, the comparison function allows us to answer both questions without requiring separate identifiers; we can use a single, general-purpose identifier. We first build a single identifier (the more specific one): it does not sort the sequences, and does include names. This identifier, as before, needs only a string match for the second use case. To answer the question of the first use case, checking if the strings match is not enough, since it can't confirm the content is the same even without a matching order, since the digests were computed using the given order. Thus, to make this comparison, you'd need to go one further step and use the comparison function. Luckily, this is very simple: you run the comparison function on the two digests, and the result immediately tells you for the sequences attribute, whether they have the same sequences in a different order, independent of names, answering your question.</p> <p>So, the vision here is that we've moved away from a quick check to see if two strings match, into a more information-rich function where you look at the output of the comparison function to answer the question. This added complexity is required if we want to use sequence collections to solve more than one problem. Admittedly, by necessity this makes the standard more complex for a single use case alone. However, it also makes the standard more general, meaning it is more likely to be used for a variety of different use cases. In other words, it maintains interoperability.</p> <p>The risk here is that users who only see a single use case could balk and say, \u201cWell, this is too complex for what we need. Our needs are solved by a simple algorithm; we don't need to define all these other terms, use a comparison function, etc.\u201d But while that may be true, this leads to fragmentation, with digests minted and useful only for a single purpose. The unfortunate truth is that we cannot have it both ways: it's simply impossible to define a single digest that can address all the comparisons we need at the level of a string match. We face a tradeoff: either define individual identifiers for each use case at the cost of interoperability, or define a more general identifier with a more complex comparison function, maintaining interoperability at the cost of increased complexity. By adding in a bit more complexity, you're able to buy into an ecosystem where a community can define and provide tools that will support the increased complexity, leading to better interoperability and ease of use in the long run. Eventually, the benefits of doing something that solves multiple problems will outweigh the additional cost over the simple solution.</p> <p>To mitigate the costs of the increased complexity of the comparison function, we've spent years optimizing and simplifying it to make it as simple as possible while retaining enough power to make it really useful. We tried to hit a sweet spot, where this information is 1) very fast to compute; 2) reasonably easy to code (my implementation is only about 150 lines of Python code); 3) simple enough to be understood relatively easily; and 4) information-rich enough to answer a huge number of comparison questions, making it very generally useful. The result returned by a comparison is a pretty understandable JSON document, which we've documented extensively in the specification (under the HowTo on comparing sequence collections). For me, the interoperability benefits and increased power are clearly worth the added complexity.</p> <p>The comparison function is extremely powerful. Even if you don't think you need it for your use case, it will open the opportunity to do much more interesting comparisons of sequence collections that go beyond exact identity. But there are two limitations with the comparison function: First, it can't answer every possible question; it can only compare to the level specified. Second, it comes with the increased cost of having to compute a function. The third and final strategy addresses both of these limitations.</p>"},{"location":"seqcols/seqcol_rationale/#3-the-layered-approach","title":"3. The layered approach","text":"<p>In some situations it may truly be necessary to answer a comparison question with a faster string-match check. For example, if you need to compare a huge number of collections, maybe the time cost of running the comparison function is intractable. This seems unlikely, since the comparison function is extremely fast, and it does not increase the algorithmic complexity... If you're doing pairwise comparisons between many sequence collections, the time complexity will grow exponentially with the number of collections; the comparison function will only be a linear factor. Nevertheless, there may be a rare case that requires a specific digest for a single comparison.</p> <p>Our third strategy is what we call the \"layered approach,\" because Sequence Collections uses a layered, recursive method of computing digests. First, each attribute is digested independently, and then these digests are re-digested to create the final identifier. These internal digests -- the first independent digest, which we call \"level 1\" -- can be useful for some questions on their own. In some cases, it may be possible to make the comparison of interest by a string comparison of the level 1 digests, bypassing the need for the comparison function. For example, if you don't care about names, you could simply compare the <code>sequence</code> attribute-level digest. This gives you the same, fast, string-match check to answer a question, but in a framework that maintains the interoperability of the primary identifier.</p> <p>To make this even more useful, the flexibility becomes complete with custom ancillary non-inherent attributes. In short, it works like this: if you really need a particular digest, then just compute it and add it to the sequence collection as a non-inherent attribute. You can then use it the way you would outside of the standard. As an example, the standard specifies a recommended ancillary attribute called <code>sorted_name_length_pairs</code>. This attribute solves a specific use case of coordinate system compatibility. Two sequence collections that have matching <code>sorted_name_length_pairs</code> attributes will have identical coordinate systems. The details of how to construct this attribute are included in the specification. We considered this use case common enough that we included this as a recommended ancillary attribute in the standard. It also provides an example of how you could construct other ancillary attributes to answer other specific questions that are not already answered by the basic comparison function or the level 1 digests. The point is that custom attributes like this can be easily added by a particular implementation to answer a specific question that is not already answered by the comparison function, or if the situation requires a string match instead of calling an external function for performance reasons. This provides ultimately flexibility for any comparison use case we have not already imagined, all within a framework that maintains interoperability of the final sequence collection identifiers.</p>"},{"location":"seqcols/seqcol_rationale/#our-proposal-for-inherent-attributes","title":"Our proposal for inherent attributes","text":"<p>These strategies are major steps toward balancing interoperability and specific use cases, but there's one last major issue before we're all the way there: we need a proposal for a minimal, universal set of inherent attributes. One that could be the catch-all. Here's the problem. In our first strategy, we described how users could change the inherent attributes in the schema, depending on a particular use case. That's great for accommodating different needs while maintaining a common framework, but it also leads to a potential challenge: If two different implementations define different inherent attributes, then the digests of the same sequence collection will not match between these two implementations. That's part of the definition of inherent. This means if people use different sets of inherent attributes, we've again lost the interoperability that we've sought. By making the specification flexible enough that people can change which attributes contribute to the digest, we also open up the possibility that people will choose different inherent attributes, costing interoperability.</p> <p>Our solution to this is that we should provide a general standard list of inherent attributes, an \"accepted\" or \"approved\" schema, as a recommended part of the specification. This will encourage people to adopt this set of attributes, preserving interoperability for as many implementations as possible. But it also maintains the flexibility for users who need to implement something that breaks that interoperability. Buying into a general-purpose standardized schema is the only way to get interoperability at the level of matching digests across use cases. Thus, the specification includes a RECOMMENDED set of inherent attributes.</p>"},{"location":"seqcols/seqcol_rationale/#dont-get-caught-up-in-the-structure","title":"Don't get caught up in the structure","text":"<p>One final important point. Sometimes people seeing the standard for the first time are confused at the way we structure sequence collections when we compute the digest. Specifically, why we structure the collection as an object of arrays, rather than an array of objects. In some use cases, it's more natural to think of a sequence with its metadata (name, etc) as an object, and a sequence collection as an array of such objects. Flipping the axis to group the information by attribute, instead of by sequence isn't the way they've thought about collections of objects and so it's unnatural, and this turns them off to the standard.</p> <p>For reasons outlined in the specification, for the actual computing of the identifier, it's important to use the array-based structure -- this is what enables us to use the \"level 1\" digests for certain comparison questions, and also provides critical performance benefits for extremely large sequence collections. But don't let this dissuade you! My critical point is this: the way to compute the interoperable identifier does not force you to structure your data in a certain way in your service -- it's simply the way you structure the data when you compute its identifier. You are, of course, free to store a collection however you want, in whatever structure makes sense for you. You'd just need to structure it according to the standard if you wanted to implement the algorithm for computing the digest. In fact, my implementation provides a way to retrieve the collection information in either structure. </p>"},{"location":"seqcols/seqcol_rationale/#sequence-collections-without-sequences","title":"Sequence collections without sequences","text":"<p>Typically, we think of a sequence collection as consisting of real sequences, but in fact, sequence collections can also be used to specify collections where the actual sequence content is irrelevant. Since this concept can be a bit abstract for those not familiar, we'll try here to explain the rationale and benefit of this. First, consider that in a sequence comparison, for some use cases, we may be primarily concerned only with the length of the sequence, and not the actual sequence of characters. For example, BED files provide start and end coordinates of genomic regions of interest, which are defined on a particular sequence. On the surface, it seems that two genomic regions are only comparable if they are defined on the same sequence. However, this not strictly true; in fact, really, as long as the underlying sequences are homologous, and the position in one sequence references an equivalent position in the other, then it makes sense to compare the coordinates. In other words, even if the underlying sequences aren't exactly the same, as long as they represent something equivalent, then the coordinates can be compared. A prerequisite for this is that the lengths of the sequence must match; it wouldn't make sense to compare position 5,673 on a sequence of length 8,000 against the same position on a sequence of length 9,000 because those positions don't clearly represent the same thing; but if the sequences have the same length and represent a homology statement, then it may be meaningful to compare the positions. </p> <p>We realized that we could gain a lot of power from the seqcol comparison function by comparing just the name and length vectors, which typically correspond to a coordinate system. Thus, actual sequence content is optional for sequence collections. We still think it's correct to refer to a sequence-content-less sequence collection as a \"sequence collection\" -- because it is still an abstract concept that is representing a collection of sequences: we know their names, and their lengths, we just don't care about the actual characters in the sequence in this case. Thus, we can think of these as a sequence collection without sequence characters.</p> <p>An example of a canonical representation (level 2) of a sequence collection with unspecified sequences would be:</p> <pre><code>{\n  \"lengths\": [\n    \"1216\",\n    \"970\",\n    \"1788\"\n  ],\n  \"names\": [\n    \"A\",\n    \"B\",\n    \"C\"\n  ]\n}\n</code></pre>"},{"location":"seqcols/sequences_from_digest/","title":"How to: Retrieve a collection given a digest","text":""},{"location":"seqcols/sequences_from_digest/#use-case","title":"Use case","text":"<p>You have a seqcol digest, and you'd like to retrieve the underlying sequence digests, or sequences themselves.</p>"},{"location":"seqcols/sequences_from_digest/#how-to-do-it","title":"How to do it","text":"<p>To look up the contents of a digest will require a seqcol service that stores the collection in a database.</p>"},{"location":"seqcols/sequences_from_digest/#1-retrieving-the-sequence-digests","title":"1. Retrieving the sequence digests","text":"<p>You can retrieve the canonical seqcol representation by hitting the <code>/collection/:digest</code> endpoint, where <code>:digest</code> should be changed to the digest in question. If all you need is sequence digests, then you're done.</p>"},{"location":"seqcols/sequences_from_digest/#2-retrieving-underlying-sequences","title":"2. Retrieving underlying sequences","text":"<p>If you need sequences, then you'll also need a refget server. Sequence collection services don't necessarily store sequences themselves; this task is typically outsource to a refget server. The seqcol server simply stores the group information, and metadata accompanying the sequences. Therefore, to retrieve the underlying sequences, you can first retrieve the sequence digests, and then use these digests to query a refget service.</p>"},{"location":"sequences/","title":"Refget Sequences v2.0.0","text":""},{"location":"sequences/#introduction","title":"Introduction","text":"<p>Reference sequences are fundamental to genomic analysis and interpretation however naming is a serious issue. For example the reference genomic sequence GRCh38/1 is also known as hg38/chr1, CM000663.2 and NC_000001.11. In addition there is no standardised way to access reference sequence from providers such as INSDC (ENA, Genbank, DDBJ), Ensembl or UCSC.</p> <p>Refget enables access to reference sequences using an identifier derived from the sequence itself.</p> <p>Refget uses a hash algorithm (by default <code>MD5</code>) to generate a checksum identifier, which is a digest of the underlying sequence. This removes the need for a single accessioning authority to identify a reference sequence and improves the provenance of sequence used in analysis. In addition refget defines a simple scheme to retrieve reference sequence via this checksum identifier.</p> <p>Refget is intended to be used in any scenario where full or partial access to reference sequence is required e.g. the CRAM file format or a genome browser.</p>"},{"location":"sequences/#design-principles","title":"Design principles","text":"<p>The API has the following features:</p> <ul> <li>The checksum algorithm used to derive the sequence identifier shall be a mainstream algorithm available standard across multiple platforms and programming languages.</li> <li>The client may request a sub-sequence, which the server is expected to honour</li> <li>Refget was designed to enable access to nucleotide sequences, however other sequences could be provided via the same mechanism e.g. cDNA, CDS, mRNA or proteins</li> <li>Optionally the API provides a retrieval of the sequence and metadata via a unique identifier</li> </ul>"},{"location":"sequences/#openapi-description","title":"OpenAPI Description","text":"<p>An OpenAPI description of this specification is available and describes the 2.0.0 version. OpenAPI is a language independent way of describing REST services and is compatible with a number of third party tools. (Note: if there are differences between this text and the OpenAPI description, this specification text is definitive.)</p>"},{"location":"sequences/#compliance","title":"Compliance","text":"<p>Implementers can check if their refget implementations conform to the specification by using our compliance suite. A summary of all known public implementations is available from our compliance report website.</p>"},{"location":"sequences/#protocol-essentials","title":"Protocol essentials","text":"<p>All API invocations are made to a configurable HTTP(S) endpoint, receive URL-encoded query string parameters and HTTP headers, and return text or other allowed formatting as requested by the user. Successful requests result in HTTP status code 200 and have the appropriate text encoding in the response body as defined for each endpoint. The server may provide responses with chunked transfer encoding. The client and server may mutually negotiate HTTP/2 upgrade using the standard mechanism.</p> <p>The response for sequence retrieval has a character set of US-ASCII and consists solely of the requested sequence or sub-sequence with no line breaks. Other formatting of the response sequence may be allowed by the server, subject to standard negotiation with the client via the Accept header.</p> <p>Requests adhering to this specification MAY include an Accept header specifying an alternative formatting of the response, if the server allows this. Otherwise the server shall return the default content type specified for the invoked method.</p> <p>HTTP responses may be compressed using RFC 2616 transfer-coding, not content-coding.</p> <p>HTTP response may include a 3XX response code and Location header redirecting the client to retrieve sequence data from an alternate location as specified by RFC 7231, clients SHOULD be configured follow redirects. <code>302</code>, <code>303</code> and <code>307</code> are all valid response codes to use.</p> <p>Range headers are the preferred method for clients making sub-sequence requests, as specified by RFC 7233.</p> <p>Requests MAY include an Accept header specifying the protocol version they are using:</p> <pre><code>Accept: text/vnd.ga4gh.refget.v2.0.0+plain\n</code></pre> <p>Responses from the server MUST include a Content-Type header. A plain text (<code>text/plain</code>) response MAY include the encoding for the invoked method and protocol version. The refget protocol reserves <code>text/plain</code> for the transfer of sequence data. A JSON response MUST include the encoding for the invoked method and protocol version for example:</p> <pre><code>Content-Type: application/vnd.ga4gh.refget.v2.0.0+json; charset=us-ascii\n</code></pre>"},{"location":"sequences/#internet-media-types-handling","title":"Internet Media Types Handling","text":"<p>When responding to a request a server MUST use the fully specified media type for that endpoint. When determining if a request is well-formed, a server MUST allow a internet type to degrade like so</p> <ul> <li><code>text/vnd.ga4gh.refget.v2.0.0+plain; charset=us-ascii</code><ul> <li><code>text/vnd.ga4gh.refget.v2.0.0+plain</code></li> <li><code>text/plain</code></li> </ul> </li> <li><code>application/vnd.ga4gh.refget.v2.0.0+json; charset=us-ascii</code><ul> <li><code>application/vnd.ga4gh.refget.v2.0.0+json</code></li> <li><code>application/json</code></li> </ul> </li> </ul>"},{"location":"sequences/#errors","title":"Errors","text":"<p>The server MUST respond with an appropriate HTTP status code (4xx or 5xx) when an error condition is detected. In the case of transient server errors (e.g., 503 and other 5xx status codes), the client SHOULD implement appropriate retry logic. For example, if a client sends an alphanumeric string for a parameter that is specified as unsigned integer the server MUST reply with <code>Bad Request</code>.</p> Error type HTTP status code Description <code>Bad Request</code> 400 Cannot process due to malformed request, the requested parameters do not adhere to the specification <code>Unauthorized</code> 401 Authorization provided is invalid <code>Not Found</code> 404 The resource requested was not found <code>Not Acceptable</code> 406 The requested formatting is not supported by the server <code>Conflict</code> 409 The document requested cannot be uniquely resolved from the provided identifier <code>Range Not Satisfiable</code> 416 The Range request cannot be satisfied <code>Not Implemented</code> 501 The specified request is not supported by the server"},{"location":"sequences/#security","title":"Security","text":"<p>Reference sequence as defined in this specification is publicly accessible without restrictions. Adapting the implementation to retrieve private/potentially sensitive genomic data is not a good use of the protocol. In any circumstances, no personal data should be shared without proper data security measures.</p> <p>Sensitive information transmitted on public networks, such as access tokens and human genomic data, MUST be protected using Transport Level Security (TLS) version 1.2 or later, as specified in RFC 5246.</p> <p>If the data holder requires client authentication and/or authorization, then the client's HTTPS API request MUST present an OAuth 2.0 bearer access token as specified in RFC 6750, in the Authorization request header field with the Bearer authentication scheme:</p> <pre><code>Authorization: Bearer [access_token]\n</code></pre> <p>The policies and processes used to perform user authentication and authorization, and the means through which access tokens are issued, are beyond the scope of this API specification. GA4GH recommends the use of the OAuth 2.0 framework (RFC 6749) for authentication and authorization.</p>"},{"location":"sequences/#checksum-calculation","title":"Checksum calculation","text":"<p>The recommended checksum algorithms are <code>MD5</code> (a 32 character HEX string) and a SHA-512-based system called <code>ga4gh</code> (a base64 URL-safe string, see later for details). Servers MUST support sequence retrieval by one or more of these algorithms, and are encouraged to support all to maximize interoperability. An older algorithm called <code>TRUNC512</code> existed in version 1.0.0 of refget but is now deprecated in favour of the GA4GH sequence checksum string. It is possible to translate between the <code>ga4gh</code> and <code>TRUNC512</code> systems however <code>TRUNC512</code> usage SHOULD be discouraged.</p> <p>When calculating the checksum for a sequence, all non-base symbols (\\n, spaces, etc) must be removed and then the rest uppercased. The allowed alphabet for checksum calculation is uppercase ASCII letters (<code>0x41</code>-<code>0x5A</code> or <code>A-Z</code>).</p> <p>Resulting hexadecimal checksum strings shall be considered case insensitive. 0xa is equivalent to 0xA.</p>"},{"location":"sequences/#refget-checksum-algorithm","title":"refget Checksum Algorithm","text":"<p>The refget checksum algorithm is called <code>ga4gh</code>. It is based on and derived from work carried out by the GA4GH VRS group. The checksum of a reference sequence string is computed as follows:</p> <ol> <li> <p>Canonicalize the sequence string by removing all non-alphabetic characters, including line terminators and other whitespace, and converting any lowercase letters to uppercase.</p> <p>(The canonicalised string then contains only uppercase ASCII letters <code>A-Z</code>.)</p> </li> <li> <p>Compute the SHA-512 digest of that canonical sequence string.</p> </li> <li> <p>Take the first 24 bytes of that digest and <code>base64url</code>-encode them.</p> <p>(This uses the URL-safe Base 64 variant described in RFC 4648 \u00a75, which uses the characters <code>A-Za-z0-9-_</code>. Because the length of the digest prefix taken is a multiple of three, the <code>=</code> pad character is never necessary.)</p> </li> <li> <p>Prepend <code>SQ.</code> to the start of the resulting 32-character text string.</p> </li> </ol> <p>Services may also implement the older <code>TRUNC512</code> representation of a truncated SHA-512 digest, which uses similar ideas to the above <code>ga4gh</code> string. See later in this specification for implementation details of the TRUNC512 algorithm and conversion between <code>ga4gh</code> and <code>TRUNC512</code>.</p> <p>For example, the <code>ga4gh</code> digest of <code>ACGT</code> is the string <code>SQ.aKF498dAxcJAqme6QYQ7EZ07-fiw8Kw2</code>.</p>"},{"location":"sequences/#namespace-of-the-checksums","title":"Namespace of the checksums","text":"<p>The requested checksum can optionally be prefixed with a namespace describing the type of algorithm being used. For example using md5 <code>md5:6aef897c3d6ff0c78aff06ac189178dd</code> and <code>6aef897c3d6ff0c78aff06ac189178dd</code> should return the same sequence and similarly using ga4gh <code>ga4gh:SQ.aKF498dAxcJAqme6QYQ7EZ07-fiw8Kw2</code> and <code>SQ.aKF498dAxcJAqme6QYQ7EZ07-fiw8Kw2</code> should return the same sequence.</p>"},{"location":"sequences/#unique-identifiers","title":"Unique Identifiers","text":"<p>Refget optionally allows the use of namespaced identifiers in place of the digest. The identifier prefixed by a namespace to form a CURIE for example:</p> <p><code>insdc:CM000663.2</code></p> <p>It is recommended that each namespaced identifier be unique within the refget implementation but if it does not resolve to a single sequence the server must respond with either:</p> <ul> <li>code 300: <code>multiple choices</code> providing a list of identifiers or sequence digests that correspond to the request;</li> <li>code 409: <code>conflict</code> indicating a conflict that cannot be resolved.</li> </ul>"},{"location":"sequences/#cors","title":"CORS","text":"<p>Cross-origin resource sharing (CORS) is an essential technique used to overcome the same origin content policy seen in browsers. This policy restricts a webpage from making a request to another website and leaking potentially sensitive information. However the same origin policy is a barrier to using open APIs. GA4GH open API implementers should enable CORS to an acceptable level as defined by their internal policy. For any public API implementations should allow requests from any server.</p> <p>GA4GH is publishing a CORS best practices document, which implementers should refer to for guidance when enabling CORS on public API instances.</p>"},{"location":"sequences/#api-methods","title":"API Methods","text":""},{"location":"sequences/#method-get-sequence-by-id","title":"Method: get sequence by ID","text":"<p><code>GET /sequence/&lt;id&gt;</code></p> <p>The primary method for accessing specified sequence data. The response is the requested sequence or sub-sequence in text unless an alternative formatting supported by the server is requested.</p> <p>The client may specify a genomic range to retrieve a sub-sequence via either the Range header OR start/end query parameters, however the Range header is the recommended method. If a sub-sequence is requested via <code>start</code>/<code>end</code> query parameters, the response must be 200 and only contain the specified sub-sequence. If a sub-sequence is requested via a Range header, the response must be one of 206 and only contain the specified sub-sequence, be 200 and contain the entire sequence (thus ignoring the Range header), or 303 redirecting the client to where it can retrieve the sequence.</p> <p>If a sub-sequence is requested, the response must only contain the specified sub-sequence. A server may place a length limit on sub-sequences returned via query parameter, queries exceeding this limit shall return <code>Range Not Satisfiable</code>.</p> <p>If <code>start</code> and <code>end</code> are set to the same value the server should return a 0-length string.</p> <p>A server may support circular chromosomes as a reference sequence, but this is not mandatory. If a reference sequence represents a circular chromosome and the server supports circular chromosomes, a sub-sequence query with a start greater than the end will return the sequence from the start location to the end of the reference, immediately followed by the sequence from the first base to the end. If the server supports circular chromosomes and the chromosome is not circular or the range is outside the bounds of the chromosome the server shall return <code>Range Not Satisfiable</code>. Otherwise if circular chromosomes are not supported, a <code>Not Implemented</code> shall be returned. Sub-sequences of circular chromosomes across the origin may not be requested via the Range header. The starting point of a circular chromosome is determined by an external authority and not by the refget implementation.</p>"},{"location":"sequences/#default-encoding","title":"Default encoding","text":"<p>Unless negotiated with the client and allowed by the server, the default encoding for this method is:</p> <pre><code>Content-type: text/vnd.ga4gh.refget.v2.0.0+plain\n</code></pre>"},{"location":"sequences/#url-parameters","title":"URL parameters","text":"Parameter Data Type Required Description <code>id</code> string Yes A string specifying an identifier uniquely associated with a sequence so that at most one sequence is returned for an id. The id can be a checksum or other unique namespaced identifier supported by the server."},{"location":"sequences/#query-parameters","title":"Query parameters","text":"Parameter Data Type Required Description <code>start</code> 32-bit unsigned integer Optional The start position of the range on the sequence, 0-based, inclusive. The server MUST respond with a <code>Bad Request</code> error if start is specified and is larger than the total sequence length. The server MUST respond with a <code>Range Not Satisfiable</code> error if start and end are specified and start is greater than end and the sequence is not a circular chromosome. Otherwise if the server does not support circular chromosomes it MUST respond with <code>Not Implemented</code> if the start is greater than the end. The server MUST respond with <code>Bad Request</code> if start and the Range header are both specified. <code>end</code> 32-bit unsigned integer Optional The end position of the range on the sequence, 0-based, exclusive. The server MUST respond with a <code>Range Not Satisfiable</code> error if start and end are specified and start is greater than end and the sequence is not a circular chromosome. Otherwise if the server does not support circular chromosomes it MUST respond with <code>Not Implemented</code> if the start is greater than the end. The server MUST respond with <code>Bad Request</code> if end and the Range header are both specified."},{"location":"sequences/#request-parameters","title":"Request parameters","text":"Parameter Data Type Required Description <code>Range</code> string Optional Range header as specified in RFC 7233, however only a single byte range per GET request is supported by the specification. The byte range of the sequence to return, 0-based inclusive of start and end bytes specified. The server MUST respond with a <code>Bad Request</code> error if both a Range header and start or end query parameters are specified. The server MUST respond with a <code>Bad Request</code> error if one or more ranges are out of bounds of the sequence. <code>Accept</code> string Optional The formatting of the returned sequence, defaults to <code>text/vnd.ga4gh.refget.v2.0.0+plain</code> if not specified. A server MAY support other formatting of the sequence. The server SHOULD respond with a <code>Not Acceptable</code> error if the client requests a format not supported by the server."},{"location":"sequences/#response","title":"Response","text":"<p>The server shall return the requested sequence or sub-sequence as a single string in uppercase ASCII text (bytes 0x41-0x5A) with no line terminators or other formatting characters. The server may return the sequence in an alternative formatting, such as JSON or FASTA, if requested by the client via the <code>Accept</code> header and the format is supported by the server.</p> <p>On success and either a whole sequence or sub-sequence is returned the server MUST issue a 200 status code if the entire sequence is returned. A server SHOULD return a 206 status code if a Range header was specified and the request was successful.</p> <p>If start and end query parameter are specified and equal each other, the server should respond with a zero length string i.e.</p> <pre><code>GET /sequence/6aef897c3d6ff0c78aff06ac189178dd?start=0&amp;end=0\n</code></pre> <p>If a start and/or end query parameter are specified the server should include a <code>Accept-Ranges: none</code> header in the response.</p> <p>If the identifier is not known by the server, a 404 status code and <code>NotFound</code> error shall be returned.</p>"},{"location":"sequences/#example-text-request-and-response","title":"Example text request and response","text":"<p>The following response has been cut for brevity.</p> <pre><code>GET /sequence/6aef897c3d6ff0c78aff06ac189178dd\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN ....\n</code></pre>"},{"location":"sequences/#method-get-known-metadata-for-an-id","title":"Method: Get known metadata for an id","text":"<pre><code>GET /sequence/&lt;id&gt;/metadata\n</code></pre> <p>Return all known names for an identifier and related metadata.</p> <p>Due to the possibility of multiple checksum algorithms being supported by a server, and potentially other known aliases for a sequence existing, this method allows clients to query all known names for a given identifier as well as fetch any associated metadata.</p>"},{"location":"sequences/#default-encoding_1","title":"Default encoding","text":"<p>Unless negotiated with the client and allowed by the server, the default encoding for this method is:</p> <pre><code>Content-type: application/vnd.ga4gh.refget.v2.0.0+json\n</code></pre>"},{"location":"sequences/#url-parameters_1","title":"URL parameters","text":"Parameter Data Type Required Description <code>id</code> string Yes A string specifying an identifier to retrieve metadata for using one of the defined checksum algorithms or a server-specific checksum algorithm."},{"location":"sequences/#request-parameters_1","title":"Request parameters","text":"Parameter Data Type Required Description <code>Accept</code> string Optional The formatting of the returned metadata, defaults to <code>application/vnd.ga4gh.refget.v2.0.0+json</code> if not specified. A server MAY support other formatting of the sequence. The server SHOULD respond with a <code>Not Acceptable</code> error if the client requests a format not supported by the server."},{"location":"sequences/#response_1","title":"Response","text":"<p>The server shall return a list of all identifiers for algorithms the server knows and the given identifier along with associated metadata. Identifiers provided by naming authorities should be provided in the <code>aliases</code> property. The server MAY return the query identifier in the list of identifiers.</p> <p>A JSON encoded response shall have the following fields:</p> <code>metadata</code> object  Container for response object.  <code>md5</code> string  md5 checksum.  <code>ga4gh</code> string    ga4gh identifier, if the server does not support ga4gh the value will be <code>null</code>.  <code>length</code> int  The length of the reference sequence.  <code>aliases</code> array of objects  Array of objects each containing one of the known aliases. The query identifier may be in this this.  <code>alias</code> string  A known alias for the query.  <code>naming_authority</code> string  The source of the alias. See Appendix 1 for a set of recommended names to use.  <p>On success and the query identifier being known to the server, a 200 status code shall be returned.</p> <p>If the identifier is not known by the server, a 404 status code shall be returned.</p>"},{"location":"sequences/#example-json-request-and-response","title":"Example JSON request and response","text":"<pre><code>GET /sequence/6aef897c3d6ff0c78aff06ac189178dd/metadata\n\n{\n  \"metadata\" : {\n    \"md5\" : \"6aef897c3d6ff0c78aff06ac189178dd\",\n    \"ga4gh\": \"SQ.Ya6Rs7DHhDeg7YaOSg1EoNi3U_nQ9SvO\",\n    \"length\": 248956422,\n    \"aliases\" : [\n      {\n        \"alias\": \"CM000663.2\",\n        \"naming_authority\" : \"insdc\"\n      },\n      {\n        \"alias\": \"chr1\",\n        \"naming_authority\" : \"ucsc\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"sequences/#method-fetch-information-on-the-service","title":"Method: Fetch information on the service","text":"<p><code>GET /sequence/service-info</code></p> <p>Return configuration information about this server implementation. See the service-info specification page for more information.</p>"},{"location":"sequences/#default-encoding_2","title":"Default encoding","text":"<p>Unless negotiated with the client and allowed by the server, the default encoding for this method is:</p> <pre><code>Content-type: application/vnd.ga4gh.refget.v2.0.0+json\n</code></pre>"},{"location":"sequences/#request-parameters_2","title":"Request parameters","text":"Parameter Data Type Required Description <code>Accept</code> string Optional The formatting of the returned metadata, defaults to <code>application/vnd.ga4gh.refget.v2.0.0+json</code> if not specified. A server MAY support other formatting of the sequence. The server SHOULD respond with a <code>Not Acceptable</code> error if the client requests a format not supported by the server."},{"location":"sequences/#response_2","title":"Response","text":"<p>The server shall return a document detailing specifications of the service implementation. A JSON encoded response shall have the following fields in addition to those specified by service-info:</p> <code>refget</code> object  Container for response object.  <code>circular_supported</code> boolean  If circular genomes are supported by the server.  <code>algorithms</code> array of enum(strings)  An array of strings listing the digest algorithms that are supported. Standard values: <code>md5</code>, <code>trunc512</code>, <code>ga4gh</code> (though others may be specified as a non-standard extension).  <code>identifier_types</code> array of strings  An array of strings listing the type identifiers supported. Values used should be the same as the one supported by identifiers.org such as <code>insdc</code>, <code>ensembl</code>, <code>refseq</code>.  <code>subsequence_limit</code> int or null  An integer giving the maximum length of sequence which may be requested using <code>start</code> and/or <code>end</code> query parameters or <code>Range</code> header. <code>null</code> values or values lower than 1 mean the server has no imposed limit."},{"location":"sequences/#example-json-request-and-response_1","title":"Example JSON request and response","text":"<pre><code>GET /service-info\n\n{\n  \"id\": \"org.ga4gh.refget\",\n  \"name\": \"Refget server\",\n  \"type\": {\n    \"group\": \"org.ga4gh\",\n    \"artifact\": \"refget-sequence\",\n    \"version\": \"2.0.0\"\n  },\n  \"description\": \"Reference sequences from checksums\",\n  \"organization\": {\n    \"name\": \"My organization\",\n    \"url\": \"https://example.com\"\n  },\n  \"contactUrl\": \"mailto:support@example.com\",\n  \"documentationUrl\": \"https://docs.myservice.example.com\",\n  \"createdAt\": \"2019-06-04T12:58:19Z\",\n  \"updatedAt\": \"2019-06-04T12:58:19Z\",\n  \"environment\": \"prod\",\n  \"version\": \"2.0.0\",\n  \"refget\": {\n    \"circular_supported\": true,\n    \"subsequence_limit\": 0,\n    \"algorithms\": [\n      \"md5\",\n      \"ga4gh\",\n      \"trunc512\"\n    ],\n    \"identifier_types\": [\n      \"insdc\",\n      \"refseq\",\n      \"ensembl\"\n    ]\n  }\n}\n</code></pre>"},{"location":"sequences/#range-headers","title":"Range headers","text":"<p>GA4GH has a standard of 0-based, half-open coordinates however Range requests as described in RFC 7233 use a unit of bytes starting at 0 and inclusive of the first and last byte specified (although other units are permitted by the RFC). For this reason care must be taken when making a request with a Range header versus start/end in the query string. Range start and end would be with respect to the byte coordinates of the sequence as if it were stored in a file on disk as a continuous string with no carriage returns.</p> <p>RFC 7233 permits multiple byte ranges in a request, for this specification only a single byte range per GET request is permitted.</p> <p>For example, given the following sequence:</p> <pre><code>CAACAGAGACTGCTGCTGACAGTGGGCGGGGGAGTAGTTTGCTTGGCCCGTGGTTGAGGA\n</code></pre> <p>And a Range header to retrieve 10bp:</p> <pre><code>Range: bytes=5-14\n</code></pre> <p>The returned sub-sequence would be:</p> <pre><code>GAGACTGCTG\n</code></pre> <p>However a start/end for the same 10bp would be:</p> <pre><code>?start=5&amp;end=15\n</code></pre> <p>A Range header to retrieve a single base pair would be:</p> <pre><code>Range: bytes=0-0\n</code></pre> <p>The returned subsequence would be the first <code>C</code> of the sequence. A URL parameter for the same region would be</p> <pre><code>?start=0&amp;end=1\n</code></pre> <p>Any formatting of the sequence a server might allow is applied after the sub-sequence is selected, for example a server that supported returning FASTA the result for the prior example could be:</p> <pre><code>&gt;9f5b68f3ebc5f7b06a9b2e2b55297403 5-14\nGAGACTGCTG\n</code></pre> <p>Any bytes added for formatting to the returned output should not be taken in to account when processing a Range request.</p>"},{"location":"sequences/#alternative-checksum-algorithms","title":"Alternative Checksum Algorithms","text":"<p>Refget implementations MUST support the <code>MD5</code> identifier space and SHOULD support the <code>ga4gh</code> identifier. Non-standard identifiers are allowed but they MUST conform to the following requirements:</p> <ol> <li>Non-standard identifiers must be based on an algorithm that uses normalised sequence content as input</li> <li>The algorithm used SHOULD be a hash function</li> <li>Non-standard identifiers must not clash with the <code>MD5</code> and <code>ga4gh</code> identifier space</li> <li>Note <code>ga4gh</code> is allowed to grow in length should collisions in the current implementation be detected</li> <li>Non-standard identifiers must not clash with other identifiers on your server i.e. they must be unique.</li> </ol> <p>Any alternative identifier scheme MUST be declared in the <code>/sequence/service-info</code> endpoint under <code>algorithms</code>.</p>"},{"location":"sequences/#ga4gh-identifier-and-trunc512-algorithm-details","title":"ga4gh identifier and TRUNC512 Algorithm Details","text":"<p>Examples on how to implement both algorithm schemes in Python and Perl are available from this site.</p>"},{"location":"sequences/#design-rationale","title":"Design Rationale","text":"<p>This non-normative section provides the details behind key API decisions.</p>"},{"location":"sequences/#checksum-input-normalisation","title":"Checksum Input Normalisation","text":"<p>Key to generating reproducible checksums is the normalisation algorithm applied to sequence input. This API is based on the requirements of SAM/BAM, CRAM Reference Registry and VMC specifications. These specifications' own normalisation algorithms are detailed below:</p> <ul> <li>SAM/BAM<ul> <li>All characters outside of the inclusive range <code>33</code> (<code>0x21</code>/<code>!</code>) through <code>126</code> (<code>0x7E</code>/<code>~</code>) are stripped out</li> <li>All lower-case characters are converted to upper-case</li> </ul> </li> <li>CRAM Reference Registry<ul> <li>Input comes into the registry via ENA</li> <li>ENA allows input conforming to the following regular expression: <code>(?i)^([ACGTUMRWSYKVHDBN]+)\\*?$</code></li> </ul> </li> <li>VRS<ul> <li>VRS requires sequence to be a string of IUPAC codes for either nucleotide or protein sequence</li> </ul> </li> </ul> <p>Considering the requirements of the three systems the specification designers felt it was sufficient to restrict input to the inclusive range <code>65</code> (<code>0x41</code>/<code>A</code>) to <code>90</code> (<code>0x5A</code>/<code>Z</code>). Changes to this normalisation algorithm would require a new checksum identifier to be used.</p>"},{"location":"sequences/#checksum-choice","title":"Checksum Choice","text":"<p>MD5 provides adequate protection against hash collisions occurring from sequences. However the consequence of a sequence derived hash collision appearing would be catastrophic as two or more sequences with different content would report to be the same entity.</p> <p>The VRS, Variation Representation Specification, is a complementary GA4GH effort to model genomic variation based on deviations from a reference sequence. Part of their work was to explore hashing algorithms. We have adopted the checksum algorithm from VRS v1, based around the SHA-512 algorithm.</p> <p>The algorithm performs a SHA-512 digest of a sequence and creates a <code>base64url</code> encoding (using RFC 4648) of the first 24 bytes of the digest. Analysis performed by VRS suggests this should be sufficient to avoid message collisions (see the VRS documentation for more details). Should a message collision occur within this scheme then the number of bytes retained from the SHA-512 checksum will be increased.</p>"},{"location":"sequences/#checksum-identifier-identification","title":"Checksum Identifier Identification","text":"<p>When a checksum identifier is given to an implementation, it is the server's responsibility to compute what kind of identifier (<code>MD5</code>, <code>ga4gh</code> or <code>TRUNC512</code>) has been given. If provided, the namespace prefix should be used to figure it out. Otherwise <code>MD5</code> and <code>TRUNC512</code> can be deduced based on length; 32 and 48 characters long respectively. <code>ga4gh</code> identifiers can be detected by searching for the string <code>SQ.</code>. Should refget officially support alternative checksum based identifiers we will describe the mechanisms to resolve their identification in future versions.</p>"},{"location":"sequences/#possible-future-api-enhancements","title":"Possible Future API Enhancements","text":"<ul> <li>Allow POST requests for batch downloads</li> <li>Formally define more sequence formatting options (e.g. FASTA, protobuf)</li> <li>Allow reference sequence checksums to be bundled together e.g. to represent a reference genome</li> <li>Support groups/collections of sequences based on checksums</li> </ul>"},{"location":"sequences/#contributors","title":"Contributors","text":"<p>The following people have contributed to the design of this specification.</p> <ul> <li>Andy Yates</li> <li>Rob Davies</li> <li>Rasko Leinonen</li> <li>Oliver Hofmann</li> <li>Thomas Keane</li> <li>Heidi Sofia</li> <li>Mike Love</li> <li>Gustavo Glusman</li> <li>John Marshall</li> <li>Matthew Laird</li> <li>Somesh Chaturvedi</li> <li>Rishi Nag</li> <li>Reece Hart</li> <li>Timothe Cezard</li> <li>Reggan Thomas</li> </ul>"},{"location":"sequences/#appendix","title":"Appendix","text":""},{"location":"sequences/#appendix-1-naming-authorities","title":"Appendix 1 - Naming Authorities","text":"<p>The specification makes no attempt to enforce a strict naming authority across implementations due to their potential heterogeneous nature. However we do encourage implementers to reuse naming authority strings where possible following nomenclature from identifiers.org. See below for examples of recommended names.</p> String Authority Description Status <code>insdc</code> INSDC Used for any identifier held in an INSDC resource (GenBank, ENA, DDBJ) Active <code>ucsc</code> UCSC Used for an identifier assigned by UCSC Genome group Active <code>ensembl</code> Ensembl Used for an identifier assigned by the Ensembl project Active <code>md5</code> MD5 Prefix used to describe digests which have gone through the MD5 algorithm Active <code>refseq</code> RefSeq Used for an identifier assigned by the RefSeq group Active <code>trunc512</code> Refget The old checksum algorithm based on SHA-512 used in v1.0.0 of refget Deprecated <code>ga4gh</code> Refget ga4gh identifier, which are prefixed by the term <code>SQ.</code>. This is the preferred naming Active <code>md5</code> Refget md5 checksum of the sequence. Active <code>vmc</code> VMC Used for when an identifier is a VMC compatible digest Deprecated"},{"location":"sequences/#appendix-2-changes","title":"Appendix 2 - Changes","text":""},{"location":"sequences/#v200","title":"v2.0.0","text":"<ul> <li>Replace refget's v1 service-info implementation with GA4GH discovery's definition of service-info</li> <li>Move code examples out into a Python notebook and a Perl script</li> <li>Replace TRUNC512 with ga4gh identifier as the default SHA-512-based hash identifier (support still available for TRUNC512)</li> <li>All checksums can be requested namespaced with their algorithm</li> <li>Optional support for namespaced identifiers to resolve sequence and metadata</li> <li>Lower cased recommended naming authority strings</li> </ul>"},{"location":"sequences/#v101","title":"v1.0.1","text":"<p>Specification link</p> <ul> <li><code>plain/text</code> responses no longer need to specify a VND. <code>application/json</code> responses continue to need to do this.</li> </ul>"},{"location":"sequences/#v100","title":"v1.0.0","text":"<p>Specification link</p> <ul> <li>First release of the specification</li> </ul>"}]}